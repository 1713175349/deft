\documentclass[letterpaper,twocolumn,amsmath,amssymb,pre,aps,10pt]{revtex4-1}
\usepackage{graphicx}% Include figure files
\usepackage{color}

\newcommand{\red}[1]{{\bf \color{red} #1}}
\newcommand{\green}[1]{{\bf \color{green} #1}}
\newcommand{\blue}[1]{{\bf \color{blue} #1}}
\newcommand{\cyan}[1]{{\bf \color{cyan} #1}}

\newcommand{\davidsays}[1]{{\color{red} [\green{David:} \emph{#1}]}}
\newcommand{\mpsays}[1]{{\color{red} [\blue{Michael:} \emph{#1}]}}

\begin{document}
\title{Applying the optimized ensemble histogram method to the
  square-well liquid}

\author{Michael A. Perlin} \author{David Roundy}
\affiliation{Department of Physics, Oregon State University,
  Corvallis, OR 97331}

\begin{abstract}
  We have applied the clever histogram method to the square-well
  liquid.
\end{abstract}

\maketitle

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-E}
  \caption{A histogram plot demonstrating the difficulty of using
    canonical Monte Carlo to simulate a system.  A simulation at a
    give temperature only provides information fo a small number of
    energy states.\label{fig:histograms}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-weights}
  \caption{The weight functions from different methods.}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-dos}
  \caption{A density of states plot demonstrating the difficulty of
    using canonical Monte Carlo to simulate a system.  A simulation at
    a give temperature only provides information fo a small number of
    energy states.\label{fig:dos}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-sample-rate}
  \caption{Inverse sampling rates from different methods.}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-scaling}
  \caption{Scaling.\label{fig:scaling}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-u}
  \caption{Specific internal energy.\label{fig:u}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-hc}
  \caption{Specific heat capacity.\label{fig:hc}}
\end{figure}

%% \begin{figure}
%%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-rdf}
%%   \caption{Radial distribution function.\label{fig:rdf}}
%% \end{figure}

As shown in Fig.~\ref{fig:histograms}, there are several difficulties
encountered when running Monte Carlo simulations at a fixed
temperature.  On the top plot, which shows three simulations run at
$\lambda = 1.3$, it is apparent that each given canonical simulation
only provides statistical information for a small number of energy
states.  The bottom plot shows an even more serious issue: when
running a simulation at a fixed temperature, it is possible to become
frozen in a state that is far from the ground state, as happened in
the $kT=0.1\epsilon$ simulation.

\begin{table*}
\input{figs/scaling-table-ww130-ff30}
\caption{Scaling data}
\end{table*}

\section{New ideas for algorithms}

\subsection{Finding maximum probability state}

Technically maximum histogram or maximum entropy.

Loop:

run iterations until we see $N$ energy changes, counting the number of
returns to the initial energy.  If there are sufficient returns, then
we are at the maximum, where sufficient returns will be determined by
a random walk (weighted?) where we compute the expected number of
returns if the walk is not weighted.  We may want to correct to get
all the way to the top.

If we are not at the top yet, we try again.  Eventually we will be.

\subsection{Finding variance of the histogram}

I assume we first ran the max-entropy/histogram algorithm, which gives
us a maximum value of $E_0$.  We then zero the histogram.

We run for a short while, and then compute the mean and standard
deviation of the energy from the histogram: $\bar E$ and $\sigma$.
While doing this, we track the number of returns to the initial state
$n$.  Now the uncertainty in the mean is given by
\begin{equation}
  \Delta \bar E \approx \frac{\sigma}{\sqrt{n}}
\end{equation}
if $\Delta \bar E \ll (E_0 - \bar E)$ then we can conclude that we are
certain where the maximum is, and how wide it is.

\subsubsection{Counting frequency of returns!}
If we are at a local maximum probability, then the frequency of return
should exceed the zero-bias frequency of return.  If we can figure out
the zero-bias frequency of return, then when we have a statistically
significant deviation, we should know what the variance is.

Once we have the variance, we can add to the weights an appropriate
gaussian to eliminate this maximum in probability.

If the rate of return is significantly smaller than expected for zero
bias, we conclude this was not actually a maximum, and we go back to
try again for a probability maximum.  If we fail twice, then maybe
there is no significant maximum.  There are subtleties here!

\subsection{Pushing down the bubbles again and again}
Once we've gotten a new set of weights (with the bubble pushed down),
we can find a new max probability, and then a new variance and push
down yet another bubble.  In each case, the ``probability'' that we
are maximizing and finding the variance of is the ratio between the
density of states and the weight function, so its log is the
difference between the logs of the density of states and the weight
function.

\davidsays{I think maybe we only want to keep ``pushing down the
  bubbles'' so long as the variance increases.  I'm afraid that if the
  variance drops we could possibly create a dangerous hole when we try
  pushing down a sharp and pointy bubble...}

\mpsays{(14 November 2014): I tried running the bubble pushing method
  with the end condition that the added log-space weight gaussian has
  a width 0.5 times that of our energy range (maximum entropy to low
  energy points), and this is precisely what happened: the simulation
  got stuck pushing down sharp and pointy bubbles. Other observations:
  the largest gaussian width ever observed was 0.27 times the energy
  range ($\lambda=1.3$, $\eta=0.3$, $N=20$). Gaussian width also
  jumped around a good deal during initialization, and I might have to
  make some figures to see what exactly happens to it during
  initialization.}

\subsection{Tracking transitions}

We can track the number of attempted transitions from each energy
state to each other energy state.  This statistic has the advantage of
being approximately independent of the weighting function used
(although it is highly dependent on the step size).  Thus we could
continue to refine this statistic, even as we are updating the weight
array.  Here are two papers that seem highly relevant to this
method~\cite{wang1999transition, wang2002transition}, but I'm not sure
if they're identical.

The ratio of density of states at two energies should be proportional
to the ratio of upgoing and downgoing transition rates between those
two energies.  Thus these transitions give us information about
$\Delta D$ (where $D$ is the density of states).

Figure~\ref{fig:transitions} below shows the transition matrix for a
20-sphere system, computed during initialization using the Wang-Landau
algorithm.

How can we use this?

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-transitions}
  \caption{A plot showing the probability of energy transitions (with
    no weighting) from given energy states.\label{fig:transitions}}
\end{figure}

\subsubsection{Generating $w$ from the transition matrix}

We should be able to generate a set of weighting functions directly
from the transition matrix.  We could do this either by optimizing for
a flat histogram, or by optimizing the round-trip rate.  Both would be
interesting to implement.

One natural approach would use linear algebra.  If the normalized
transition matrix is $T$, then the density of states is given by
\begin{equation}
  D = TD
\end{equation}
By solving this equation for $D$, we could find a good approximation
for $w$ to obtain a flat histogram.

I expect that we could use similar math to the optimized ensemble
folks to optimize the round-trip rate.

\subsubsection{Sampling the transition matrix (more) directly}

If our goal (in initialization) is to sample the transition matrix
(from which we compute the optimal weights), we could use a much more
direct approach.  When computing a histogram, we are forced to perform
a complete random walk.  However, the transition matrix can be
computed at a given energy \emph{without sampling any other energies}!

We can start at a given energy, sample the transition matrix at that
energy to our satisfaction, i.e. until we have observed transitions in
both directions a statistically significant number of times.  Then we
would proceed to the next energy and sample it, and so forth.  There
is a weakness in that we may not be able to sample all states at a
given energy without going to other energies.  But we could repeat
this procedure many times to accomplish this.  In a way, this
algorithm is reminiscent of Wang-Landau, which ``forces'' the
simulation to sample a whole slew of different energies, but in our
case we would be able to put strict bounds on how long it spends
sampling each energy, and could reasonably keep the code from wasting
huge amounts of time sampling the high-entropy states.

\bibliography{paper}% Produces the bibliography via BibTeX.

\end{document}
