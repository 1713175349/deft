\documentclass[letterpaper,twocolumn,amsmath,amssymb,pre,aps,10pt]{revtex4-1}
\usepackage{graphicx} % Include figure files
\usepackage{color}
\usepackage{nicefrac} % Include for inline fractions

\newcommand{\red}[1]{{\bf \color{red} #1}}
\newcommand{\green}[1]{{\bf \color{green} #1}}
\newcommand{\blue}[1]{{\bf \color{blue} #1}}
\newcommand{\cyan}[1]{{\bf \color{cyan} #1}}

\newcommand{\davidsays}[1]{{\color{red} [\green{David:} \emph{#1}]}}
\newcommand{\jpsays}[1]{{\color{red} [\blue{Jordan:} \emph{#1}]}}
\newcommand{\tssays}[1]{{\color{red} [\cyan{Tanner:} \emph{#1}]}}

\begin{document}
\title{Stochastic Approximation Monte Carlo with a Dynamic Update
Factor (SAD)
}

\author{Jordan K. Pommerenck} \author{Tanner T. Simpson}
\author{Michael A. Perlin} \author{David Roundy}
\affiliation{Department of Physics, Oregon State University,
  Corvallis, OR 97331}

\begin{abstract}
  We present a novel Monte-Carlo algorithm based on the Stochastic
  Approximation Monte Carlo (SAMC) algorithm for directly calculating
  the density of states. The proposed method is Stochastic
  Approximation with a dynamic update factor (SAD-$\gamma$ or SAD)
  which dynamically adjusts the update factor $\gamma$ during the course of
  the simulation. We test this MC method on the square-well fluid and
  compare the convergence time and average entropy error for a variety of weight-based
  Monte-Carlo methods. SAD rapidly converges to the
  correct density of states without the need for the user to specify an
  arbitrary tunable parameter $t_0$ as in the case of SAMC.
\end{abstract}

\maketitle

\section{Introduction}
Over the past several decades a number of flat histogram Monte-Carlo
simulation algorithms have been developed which calculate the
thermodynamic properties of various systems for all temperatures.  The
development began with the original histogram method, which used a
single canonical Monte Carlo simulation to predict properties for
nearby temperatures~\cite{ferrenberg1988new}.  For large systems this
approach is limited to a narrow temperature range because a single
canonical simulation explores only a small range of energies.  This
led to a variety of ``broad'' (or ``flat'') histogram
methods~\cite{penna1996broad, penna1998broad, swendsen1999transition,
  wang2001determining, wang2001efficient, trebst2004optimizing}, which
attempt to explore a wide range of energies.  Another benefit, in
contrast with low-temperature canonical Monte Carlo, is that these
approaches cannot be trapped in a local energy minimum.

Wang and Landau introduced an algorithm (WL) that uses an update
factor and a statistical histogram to compute the density of states of
a given system~\cite{wang2001determining, wang2001efficient}.  While
the method is incredibly powerful, it has a few disadvantage.
Firstly, it requires the user to select a number of parameters,
including the range of energies to be studied~\cite{landau2004new}.
This adds an additional hurdle to its application to a variety of
systems.  The simulation violates detailed balance, with the size of
the violation decreasing with time, which complicates convergence
analysis.  In fact, the error in a WL computation the error has been
demonstrated to saturate at a non-zero value~\cite{yan2003fast},
i.e. the method does not converge to the true density of
states~\cite{belardinelli2007wang, belardinelli2007fast,
  belardinelli2008analysis, belardinelli2014intrinsic,
  singh2012density}.  
%
Belardinelli and Pereyra demonstrated that allowing the update factor
to decrease faster than $1/t$ leads to
nonconvergence~\cite{belardinelli2007wang}.
%% Around the same time, the convergence rate for WL was being
%% explored~\cite{zhou2005understanding,lee2006convergence,
%%   belardinelli2007wang}.
%
This leads to their $1/t$-WL algorithm which ensures that the error
continues to decrease asymptotically as
$1/\sqrt{t}$~\cite{belardinelli2008analysis}.

Liang, Liu, and Carrol began to consider whether WL could be considered
a special case of Stochastic Approximation whose convergence could be
mathematically proven~\cite{liang2006theory, liang2007stochastic}. In
2007, Liang et al.~\cite{liang2007stochastic} argued that WL can be
considered a form of Stochastic Approximation Monte Carlo (SAMC). While
SAMC can guarantee convergence, the method still has a system specific
user-defined variable which makes applying this algorithm to arbitrary
systems difficult.

In this work, we have developed a novel algorithm based on SAMC that
does not require user-defined inputs and therefore should be easily
applicable to a given system.  We call this method SAD (Stochastic Approximation
with a dynamic update factor), and will discuss it in detail in the methods
section. We compare it along with several flat histogram methods
which include WL, $1/t$-WL, and SAMC.

We consider the square-well fluid i.e. a system of particles whose
interactions are governed by a square-well
potential~\cite{singh2003surface, barker2004perturbationSW}.  The
square-well potential is an ideal test-bed as it is the simplest model
that ensures both attractive and repulsive forces are experienced by
interacting particles~\cite{barker1967-SW-perturbation, vega1992phase}.
The potential $U(\textbf{r})$ for such a system is given by
\begin{equation}
 U(\textbf{r})=\begin{cases} \infty &
 \lvert\textbf{r}\rvert< \sigma\\-\epsilon &
 \sigma<\lvert\textbf{r}\rvert<\lambda\sigma\\0 &
 \lvert\textbf{r}\rvert > \lambda\sigma\end{cases}
\end{equation}
where $\sigma$ is the hard-sphere diameter of the particle, $\lambda$ is the
reduced range of the potential well, and $\epsilon$ is its depth.

The organization of this paper is as follows: In Section~\ref{sec:histogram}, we
describe in detail flat-histogram methods.  Section~\ref{sec:weight} outlines
weight-based histogram methods used in this work.

Section~\ref{sec:sad} introduces the dynamic stochastic approximation method.
In Section~\ref{sec:results}, we discuss the performance of the various methods
applied to the model system.

In this work, We compare a variety of flat histogram
methods.  We outline the general workings of each algorithm that we
developed in detail while summarizing algorithms that were developed in other
works.  The following methods are introduced and applied to the
square-well fluid: Wang-Landau (WL), $1/t$-Wang-Landau ($1/t$-WL), Stochastic
Approximation Monte-Carlo (SAMC), and Stochastic Approximation with a
dynamic $\gamma$ (SAD).

\section{Flat histogram methods}\label{sec:histogram}

The goal of flat histogram methods (also called \emph{broad histogram}
or \emph{multicanonical} methods) is to simulate each energy with
similar accuracy, so as to accurately determine the density of states
over a broad range of energies.  Once the density of states is known,
a number of thermodynamic quantities---such as heat capacity or
internal energy---can be easily computed for any temperature.
Properties that require more information---such as a spatial
correlation function or a response function---can still be computed
for any temperature, provided statistics are collected for each
individual energy, which can then be reweighted for any temperature.

Each flat histogram Monte Carlo method uses a given set of ``moves''
which change a system and should satisfy detailed balance.  Each
algorithm differs in how it determines the probability of accepting a
move, and in what additional statistics (if any) must be collected in
order decide on that probability.  The methods we compare belong to the
family of weight-based methods, which only require a set of
weights $w(E)$ be collected.

Weight-based methods calculate
the density of states $D(E)$ for discrete energy
levels~\cite{haber2018performance}. For this reason, energy binning
becomes an important consideration for complex continous systems.
Energy bins are typically of uniform size for the entire energy
continuum~\cite{fasnacht2004adaptive}. Some methods such as
AdaWL~\cite{koh2013dynamically}
employ a tunable mechanism for controlling the binning for low entropic
states in order to ensure the exploration of all energies.  The method
introduced in this paper is designed to scale appropriately as bin size
is changed, but we do not test this scaling, as we use a system with
discrete energy levels.

\section{Weight-based methods}\label{sec:weight}

We will begin by introducing three related weight-based methods which
rely on a weight function $w(E)$, noting that the method proposed in
this paper (discussed in Section~\ref{sec:sad}) also falls in this
category.  In these algorithms, the probability of accepting a move is
given by
\begin{equation}
	\mathcal{P}(E_\text{old} \rightarrow E_\text{new})
	= \min\left[1,\frac{w(E_\text{old})}{w(E_\text{new})}\right]
\end{equation}
the probability biases the simulation in favor of energies with low weights.
A set of weights that are proportional to the density of states of the
system $D(E)$ will result in an entirely flat histogram.  Thus
flat histogram is a criteria for convergence for these methods.  To avoid
overflow error , since the weights may vary over more than a
few hundred orders of magnitude, the natural logarithm of the weights are used
in this work.  Since $D(E) = S(E)$, the logarithm of the weights
can be thought of as an approximation of the entropy.

Each weighted density approach uses a random walk in energy space to
estimate the density of states.  The core of these approaches
is to continuously update the weights at each step of the simulation
\begin{equation}
	\ln{w_{t+1}(E)}=\ln{w_{t}(E)}
	+\gamma
\end{equation}
where $t$ is number of the current move, and $\gamma$ is an update
factor.  This update causes the random walk to avoid energies that
have been frequently sampled, leading to a rapid exploration of energy
space.  This approach, however, violates detailed balance, due to the
acceptance probabilities changing with each move.  The severity of
this violation decreased as we decrease $\gamma$.  The various
weight-based methods discussed here differ in how the decreasing of $\gamma$
is scheduled.

\subsection{Wang-Landau}

Wang-Landau's approach~\cite{wang2001efficient,wang2001determining,
  landau2014guide} begins with $\gamma=1$, and then decreases $\gamma$
in discrete stages.  We track the number of steps at each energy during
each stage in a histogram.  When that histogram is sufficiently flat,
$\gamma$ is decreased by a specified factor, which is usually
$\frac12$.  The flatness is defined by the ratio between the minimum
value of the histogram and its average value.  When this flatness
reaches a specified threshold (typically 0.8), the $\gamma$ value is
decreased.  This approach requires that the energy range of interest
be known in advance, and difficulties can occur with this flatness
criteria due to the fact that some energies in this energy range might
never be sampled~\cite{haber2014transition}.  The entire process is
repeated until $\gamma$ reaches a desired cutoff.

The Wang-Landau approach thus has three parameters that need be
specified: the factor by which to decrease $\gamma$ when flatness is
acheived, the flatness criterion, and the cutoff that determines when
the computation is complete.  In addition, an energy range (or more
specifically, a set of energies) must be supplied, so that the
flatness criterion can be defined.

While this approach
is very efficient and has been widely used, it suffers a few
shortcomings.  Firstly, the set of energies must be specified
~\cite{wang2001efficient, schulz2003avoiding, yan2003fast}, which may
require multiple simulations. Secondly, while Wang-Landau converges quickly,
it does not in general converge to the true density of states
~\cite{belardinelli2008analysis, zhou2008optimal}.  It can and does decrease
$\gamma$ so quickly that it will never (for any cutoff value) decrease the
error in the density of states beyond a given value.

\subsection{$1/t$-Wang-Landau}
The Wang-Landau algorithm's error saturation can be corrected by
modifying the update factor such that it does not decrease too quickly.
Belardinelli and Pereyra raised the question as to whether the update
factor should be decrease by $\nicefrac12$ or some other factor for
each update~\cite{belardinelli2007fast}. They implemented a schedule
that enforced if the update factor $\gamma$ is less than
$\nicefrac{N_S}{t}$ then the update factor is set to
$\nicefrac{N_S}{t}$ and the histogram is no longer tracked. Employing
this scheduler, they found that the error saturation is avoided since
the correct density of states is approached asymptotically as
$t^{-\frac12}$~\cite{belardinelli2008analysis}. Zhou et. al further
confirmed that the WL algorithm never converges exponentially and
succesfully bounded the statistical error between
$t^{-\frac12}$ and $1/t$~\cite{zhou2008optimal}.

Schneider et. al outlines minor refinements to the $1/t$-WL algorithm
including $\nicefrac{N_E}{t}$ scaling and switching from standard WL to
$1/t$-WL when the update factor $\gamma <
\nicefrac{N_E}{t}$~\cite{schneider2017convergence}. As per the original
$1/t$ implementation~\cite{belardinelli2007fast}, all energy states $N_E$
are required to be visited at least once $H(E) \neq 0$ effectively
avoiding the concept of `flatness'.


\subsection{SAMC}
The Stochastic Approximation Monte Carlo (SAMC) algorithm addresses
the lack of convergence of Wang-Landau's approach with a simple
schedule by which the update factor $\gamma$ is continuously
decreased~\cite{liang2007stochastic, werlich2015stochastic,
  schneider2017convergence}.  The update factor is defined in the
original implementation~\cite{liang2007stochastic} in terms of an
arbitrary tunable parameter $t_0$.
\begin{align}
\gamma_{t}^{\text{SA}} =\frac{t_0}{\max(t_0,t)}
\end{align}
where as above $t$ is the number of moves that have been attempted.
The major advantage that SAMC offers is its proven convergence.
Provided the update factor satisfies
\begin{align}
\sum_{t=1}^\infty \gamma_{t} = \infty \quad\textrm{and}\quad
\sum_{t=1}^\infty \gamma_{t}^\zeta < \infty
\end{align}
where $\zeta \in \{1,2\}$, Liang has shown that the weights are proven
to converge to the true density of states~\cite{liang2006theory,
  liang2007stochastic}.  In addition, the energy range need not be
known a priori.  The time to converge depends only on the choice of
parameters $t_0$.  Unfortunately, there is no
prescription for finding an acceptable value for $t_0$, and
while the algorithm formally converges, for a poor choice of $t_0$
that convergence can be far too slow to be practical.
Liang et al. give a rule of thumb in which $t_0$
is chosen in the range from $2N_S$ to $100N_S$ where $N_S$ is the number
of energy bins~\cite{liang2007stochastic}.  Schneider et al. found
that for the
Ising model this heuristic is helpful for small spin
systems, but that larger systems require an even higher $t_0$
value~\cite{schneider2017convergence}.  Also, we find that for the square-well
fluid $t_0$ in some cases needs to be two orders of magnitude higher
than the rule of thumb of $100N_S$.

Werlich \emph{et al.} proposed scaling the SAMC $\gamma_t$ by a factor
$\gamma_0$.  While this may result in an improved rate of convergence,
it adds yet another parameter that must be empirically determined, and
we have not explored this degree of freedom.

\subsection{SAMC Convergence}\label{sec:samc-convergence}
The difficulty in using the SAMC method lies in identifying an appropriate
value for $t_0$.  A value that is either too high or too low will result
in slow convergence to the true entropy.  It is instructive to consider
separately values of $t_0$ that are too low or too high.

We can place a rigorous \emph{lower} bound $t_{\min}$ on the number of
moves required to find the true energy by considering the total change
that needs to be made to the entropy.
\begin{align}
  \Delta S_{\text{tot}} \equiv \sum_E S(E) - S_{\min}
\end{align}
The minimum number of moves that could converge the entropy is
the number of moves that will enable the above total entropy change,
which we can approximate using an integral:
\begin{align}
   \Delta S_{\text{tot}} &= \sum_{t=0}^{t_{\min}} \gamma_t \\
  % &= \int_0^{t_{\min}} \gamma(t)dt \\
  &= t_0 + \int_{t_0}^{t_{\min}} \frac{t_0}{t}dt
  \\
  &= t_0\left(1 + \ln\left(\frac{t_{\min}}{t_0}\right)\right)
\end{align}
Solving for ${t_{\min}}$ we find that
\begin{align}
  {t_{\min}} &= t_0 e^{\frac{\Delta S_{\text{tot}}}{t_0} - 1}
\end{align}
which means that the minimum time to convergence grows exponentially
as $t_0$ is made smaller.  You \emph{seriously} don't want to underestimate
$t_0$!

One might reasonably choose to err on the high side when selecting a $t_0$.
The rate of convergence is harder to estimate when $t_0$ is large, but
in general $\gamma_t$ itself forms a lower bound on the accuracy with which
the entropy may be known, with an unknown prefactor which is roughly
the coherence time of the Monte Carlo simulation.  Since $\gamma_t$ is
given by $t_0/t$, the time to converge to a given accuracy is increased
in proportion to the ratio by which we overestimate $t_0$.  Thus, while
it is exponentially painful to underestimate $t_0$, overestimating by
several orders of magnitude is also not acceptable.  We should note
that these extreme limiting cases do not preclude the possibility that there
is a wide range of $t_0$ values that lead to an acceptable convergence
rate.

\section{SAD Algorithm}\label{sec:sad}
The Stochastic Approximation with Dynamic update factor (SAD) method
is a variant of the SAMC
Algorithm that attempts to dynamically choose the modification factor
rather than relying on system dependent parameters such as $t_0$ or
$\gamma_0$.  There is an immediate advantage of such an algorithm where
parameters are chosen independent of system size or type. Each
Flat-Histogram method has unique advantages and disadvantages.
Wang-Landau requires an energy range for initialization.  SAMC removes
this energy range requirement but requires simulating every possible
energy. Our proposed method SAD requires the user to input
$T_\text{min}$, the lowest temperature of interest,
which is an immediate disadvantage of the method. However,
setting a physical parameter such as a minimum temperature
$T_\text{min}$ is possibly easier for a user
than determining in advance either an energy range or some unphysical
parameter $t_0$.

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/N50-lndos-comparison}
  \caption{The entropy of a square well fluid with 50 atoms and filling fraction
        0.3 as the green line.  The green hatched area reflects the
        minimum entropy change needed to converge to the true value.
        The light blue area is the quadratic approximation
        for the change in entropy.  The vertical dotted lines represent
        the energy corresponding to $T=1/3$ and $T=\infty$.}
  \label{fig:entropy-cartoon}
\end{figure}

While for SAMC the update factor is defined in the original
implementation, for SAD the update factor $\gamma_{t}^{\text{SAD}}$ is
thought of as $\nicefrac{\text{d}S}{\text{d}t}$. This tells us that
the SAMC parameter
$t_0$ should have dimensions of entropy.
We begin with an estimate of the average value of the entropy (relative
to the lowest entropy at $T_{\min}$).  If we assume a quadratic
dependence on energy (see Fig.~\ref{fig:entropy-cartoon}), this is given by
\begin{align}
\langle S\rangle \approx \frac13 \frac{E({T=\infty}) - E(T_{\min})}{T_{\min}}
\end{align}
We approximate this energy difference by
$E_H -E_L$ where $E_H$ and $E_L$ are defined below.
The entropy numerator of the update factor
in general should scale with the total number of interesting
energy states $N_S$, since updates to the weights are distributed between
that many energy states.  The product $N_S\langle S\rangle$ is the
total change of entropy required (starting from constant weights) to find
the true entropy, and puts a lower bound on the convergence time.
After \emph{long} times, when
all the energies have been long ago found,
we wish for a lower update factor in order to more rapidly refine the remaining
error in entropy.  We track the time at which we first visted each possible
energy.  We define $t_L$ to be the last time that we encountered an energy
that we currently believe is in the energy range of interest, so a $t\gg t_L$
we feel confident that we have established the true energy range of interest. We
gradually transition to a lower update factor (but still asymptotically scaling
as $\gamma_t \propto 1/t$ to ensure eventual convergence).  Finally,
we wish for an update factor that is \emph{never} greater than 1, because
a very large update factor could introduce errors in entropy that may take
many iterations to remove.  The SAD
expression for $\gamma_t$ which incorporates these ideas is:
\begin{align}
  \gamma_{t}^{\text{SAD}} =
     \frac{
       \frac{E_{H}-E_{L}}{T_{\text{min}}} + \frac{t}{t_L}
     }{
       \frac{E_{H}-E_{L}}{T_{\text{min}}} + \frac{t}{N_S}\frac{t}{t_L}
     }
\end{align}
This factor asymptotically has the same $1/t$ behavior as the original
SAMC algorithm;
however now every time a new energy is found to be important
during the course of
the simulation, the update factor will experience a jump. This behavior
allows SAD to dynamically prevent the update factor from decreasing too
rapidly.

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/gamma-n256}
  \caption{The update factor $\gamma_t$ for $N=256$ versus
    iteration for four different methods: WL, $1/t$-WL, SAMC, and SAD}
    \label{fig:gamma-vs-t}
\end{figure}
Figure~\ref{fig:gamma-vs-t} compares $\gamma_t$ for the related methods
SAD, WL, $1/t$-WL, and SAMC.  For SAMC, $\gamma_t$ remains constant
before dropping as $1/t$.  WL $\gamma_t$ remains at 1 for many iterations,
and then decreases very rapidly, with $1/t$-WL behaving similarly before
transitioning to a $1/t$ behavior.
The update factor for SAD fluctuates
dynamically around a value less than 1 for early MC moves, and then
decreases as approximately $1/t$ while continuing to fluctuate as 
new energies are found to be important.  At late times, the SAD $\gamma_t$
decreases as approximately $1/t^2$ until asymptoting to $N_S/t$, which
is the same as $1/t$-WL.

Since SAD does not explore \emph{all} energy states it needs determine
what energy range corresponds to the temperature range of interest,
defined by $T_{\min}<T<\infty$.
The
simulation is responsible for determining and updating this energy
range.
Given the true entropy $S(E)$, we can define the interesting energy
range as
%\begin{align}
  $E(T_{\min}) <E< E(T=\infty)$
%\end{align}
where $E(T)$ is the energy that maximizes $S-E/T$.  In the course of the
simulation this precise energy is challenging to evaluate accurately.
In order to ensure that we sample this entire energy range adequately,
we define two energy limits:  a high energy $E_H$ and a low
energy $E_L$, which define the range over which the energy histogram
is made flat. At move $t$, $E_H$ and $E_L$ are the greatest and lowest
energy that has prior to that move
had the highest histogram value (i.e. been visited the most times) in
the course of the simulation.
This definition results in a ``ratcheting'' effect, in which $E_H$
may only increase, while $E_L$ may only decrease over the course of the
simulation, which results in a conservative estimate of the range of
energies that need be sampled.

During the simulation when considering a move outside of the energy
range $E_L \le E \le E_H$, Boltzmann weights are used.  If $E\ge E_H$,
the weight is taken to be
\begin{align}\label{eq:highw}
  w(E>E_H) &= w(E_H),
\end{align}
which corresponds to an infinite temperature.
This choice ensures that
if the maximum in entropy is at an energy $E_{\max}>E_H$, then the energy
$E_{\max}$
will eventually have the highest number of counts and the ratcheting will
result in $E_H\ge E_{\max}$ .
At lower energies, Boltzmann weights corresponding to the minimum temperature
are used:
\begin{align}\label{eq:loww}
  w(E<E_L) &= w(E_L)e^{-\frac{E_L-E}{T_{\min}}}.
\end{align}
This choice has the result that if the energy $E_{\min}$ at which the free
energy at $T_{\min}$ is minimized is less than $E_L$, the lower energy
limit will ratchet down to include $E_{\min}$.
Each time we change the value of $E_H$ or $E_L$, the weights within the
new portion of the interesting energy range are set to the expressions
in Equations~\ref{eq:highw} and~\ref{eq:loww}.

\paragraph{Energy binning with SAD}

A significant advantage of SAD over other flat histogram methods is
that energy binning is solved dynamically. SAD should perform
independently for a reasonable range of binning because $\gamma \propto
\nicefrac{N_S}{t}$.  As the number of energy states found $N_S$
increases (fine binning), the time spent $t$ in each bin will decrease.
This argument explains why SAD should perform well for complex systems
in terms of binning. SAMC has a prefactor $\gamma_0$ to aid in a
similar way but this adds yet another parameter for the user to choose.

\paragraph{Scaling, step size, and SAD}
Methods such as WL can fail to converge for complex systems in part
because the same random MC moves are performed for the entire energy
range.  The main problem lies in trying to determine how to optimize
the step size such that the time to explore all energies is reasonable.
Small step sizes lead to improved sampling at the expense of slow
convergence. Koh et al. proposed multiplying $\gamma$ by an energy
dependent factor in order to improve the converge of WL in regards to
convergence and scaling~\cite{koh2013dynamically}.  Because the update
factor for SAD has an energy dependent factor already present (and
dynamically adjusted), we expect that SAD will perform consitently
scaling free for a reasonable choice of step size.  SAD should perform
extremely well compared with other flat histogram methods when applied
to complex continuous systems such as polymers/proteins and spin models.

\subsection{Convergence}
Because SAD scales as $1/t$ for large $t$, it satisfies the SAMC
convergence criteria.  However, we also want to ensure that SAD avoids
the exponentially poor convergence time that SAMC suffers when $t_0$ is
small as discussed in Section~\ref{sec:samc-convergence}.
To place a lower bound on the convergence of SAD, we will again
approximate the total entropy change using an integral.  To sidestep
the dynamical nature of SAD, we ensure that \emph{after discovering the
last important energy} at time $t_L$ the entropy change
$\Delta S_{\text{tot}}$ can be made in a reasonable number of iterations.
Thus we integrate the update factor from $t_L$ to some minimum time
$t_{\min}$ necessary for convergence.  Technically this is not a lower
bound as in the case of SAMC because the algorithm will already have a
reasonable approximation for the entropy.
\begin{align}
\Delta S_{\text{tot}} &= \int_{t_L}^{t_{\min}}
     \frac{
       3\langle S\rangle + \frac{t}{t_L}
     }{
       3\langle S\rangle + \frac{t}{N_S}\frac{t}{t_L}
     } dt \\
&\approx 3\Delta S_{\text{tot}} \left(1-\frac{t_L}{t_{\min}}\right)
\end{align}
where we assumed that $t_L\gg N_S\langle S\rangle$, which is to say
that it takes a long time to find all the interesting energy states.
Furthermore, we omitted the $\frac{t}{t_L}$ in the numerator, which is
of order unity.  This omission only makes our bound more conservative,
and enables us to solve to find that
\begin{align}
  t_{\min} &= \frac32 t_L
\end{align}
which means that we only need to run fifty percent longer after we
discover the final interesting energy state in order to have made
sufficient updates to \emph{possibly} converge to the true entropy.

\section{Results}\label{sec:results}

\begin{figure*}
  \includegraphics[width=0.5\textwidth]{figs/s000/periodic-ww1_30-ff0_30-N50-gamma-and-entropy-error-default.pdf}%
\includegraphics[width=0.5\textwidth]{figs/s000/periodic-ww1_30-ff0_30-N50-gamma-and-entropy-error-slow.pdf}
  \caption{(a) The average entropy error for each MC method for $N=50$,
               $\delta_0 = 0.05\sigma$, $\eta = 0.3$, and $T_{\min} = 1/3$
               as a function of number of iterations run.  The error is
               averaged over 8 independent simulations, and the best
               and worst simulations for each method are shown as a
               semi-transparent shaded area, and
           (b) the update factor $\gamma_t$ versus iteration number
               for the same simulations.
           (c) The average entropy error for each MC method for $N=50$,
               $\delta_0 = 0.005\sigma$, $\eta = 0.3$, and $T_{\min} = 1/3$
               as a function of number of iterations run, and
           (d) the update factor $\gamma_t$ versus iteration number
               for the same simulations.
  }\label{fig:fast-slow-gamma}
\end{figure*}

For the square-well fluid, our simulations treat two systems.  The first is
a smaller simulation with a particle number
of 50, a well-width of $\lambda = 1.3$, and a volume
corresponding to a filling fraction of $\eta = 0.3$. The second system
is larger, with a particle number
of 256, a well-width of $\lambda = 1.5$, and a volume
corresponding to a filling fraction of $\eta = 0.17$.
For each system we use a reasonable displacement distance $\delta_0 = 0.05\sigma$,
and for the smaller system we also use an unreasonably small displacement
distance of $0.005\sigma$. The simulations explore the
energy space of the systems with minimum reduced temperatures of
$T_{\text{min}} = 1/3$ for simulations of the smaller system, and $T_{\min}=1$
for the larger system.
All simulations lead to the minimum important
energy $E_{\min}$ and maximum entropy energy $E_{\max}$
being calculated (with the exception of the WL methods where both
of these parameters are needed a priori).  In the sections below, we
examine the systems parameters impact on the convergence of the methods.

The SAMC simulations
computed the density of states for the entire range of possible
energies.  The SAD simulations determined the energy range of interest
dynamically as described above, based on a specified $T_{\min}$.  For
the WL and $1/t$-WL simulations, we constrained the simulation to
remain in the energy range corresponding to $T_{\min} < T < \infty$,
as determined by a previous SAMC simulation.  Thus the WL and $1/t$-WL
simulations were given extra information that in practice would not be
available without additional computational effort, and the SAMC simulations
computed the entropy over the entire range of possible energies, which
required more effort.

We use the average entropy error vs moves as a metric to compare
simulation runtimes and overall convergence. The overall accuracy
is determined by examining the fractional error of a particular method to
a precise reference system. For each simulation, the reference system
is chosen to be the final output of a SAMC simulation with a fixed interesting energy range.
We note this because SAMC does not require an energy range as an input parameter; however,
by fixing the energy range to for SAMC, a smaller $t_0$ can be chosen considerably reducing the
simulation time.
\begin{align}
\epsilon_\text{avg} = 1 - \bigg\lvert\frac{S_\text{method}}{S_\text{ref}}\bigg\rvert
\end{align}
Simulations that take more iterations to reach the same entropy error as a
different method take longer to converge. This behavior is seen in
Fig.~\ref{fig:fast-slow-gamma}.

\subsection{A periodic system with 50 atoms}

For this simulation, we chose a minimum reduced
temperature of $1/3$, which corresponds to an interesting energy range
from $-248$ to $-120$.  The entropy of this system is shown in
Fig.~\ref{fig:entropy-cartoon} above, which shows that over this
energy range the entropy differs by 198, corresponding to a ratio of
$10^{86}$ between the highest and lowest density of states.

In order to explore the effect of simulation details on convergence,
we consider two values for the mean distance by which atoms are moved
during a Monte Carlo step.  We began with a reasonable displacement
distance of $\delta_0 = 0.05\sigma$, which corresponds to an
acceptance rate of 38\%.  We further ran simulations with a much
smaller displacement distance of $\delta_0 = 0.005\sigma$, which
resulted in an acceptance rate of 86\%, which we expected converge
more slowly.

%\begin{figure}
%  \includegraphics[width=\columnwidth]{figs/s000/periodic-ww1_30-ff0_30-N50-entropy-error-default.pdf}
%  \caption{The average entropy error for each MC method is shown for $\eta = 0.3$ and $T_{\min} = 1/3$ with
%  a fixed energy range SAMC $t_0 = 10^3$ serving as the reference.}\label{fig:N50-ff0.3-avg-error}
%\end{figure}
%\begin{figure}
%  \includegraphics[width=\columnwidth]{figs/s000/periodic-ww1_30-ff0_30-N50-entropy-error-slow.pdf}
%  \caption{The average entropy error for each MC method is shown for $\eta = 0.3$ and $T_{\min} = 1/3$ with
%  a fixed energy range SAMC $t_0 = 10^3$ from Fig.~\ref{fig:N50-ff0.3-avg-error}
%  serving as the reference.}\label{fig:N50-ff0.3-avg-error-slow}
%\end{figure}

Figure~\ref{fig:fast-slow-gamma}a shows the average error in the
entropy as a function of time for this system with the reasonable
displacement distance of $\delta_0 = 0.05\sigma$.  The solid lines
represent the average of the absolute value of the error in the
entropy averaged over eight simulations using different random number
seeds.  The range of average errors for each simulation is shown as a
shaded region around its mean error.  By the time $10^8$ moves have
been made all but the SAMC simulation with the shortest $t_0$ have
begun to converge as $1/\sqrt{t}$.  We then see the WL error begin to
saturate around $10^{10}$ moves.

Figure~\ref{fig:fast-slow-gamma}b shows the average error in
the entropy as a function of time for this system with the
unreasonably small displacement distance of $\delta_0 = 0.005\sigma$.
%
The smaller translation scale causes all methods to take additional
time to explore all energies. Based on random walk scaling, a ideal
method should scale roughly as $\delta_0^{-2}$ in the limit of small
$\delta_0$, that is, one order of magnitude in the displacement
distance should result in two order of magnitude increase in
convergence time. SAMC simulations with a $t_0$ value that rapidly
converged for $\delta = 0.05\sigma$ do not converge at all for a
translation scale of $\delta = 0.005\sigma$. SAD, WL, and $1/t$-WL
handle the shift in displacement distance and converge roughly as
expected.

The methods SAD, WL, and $1/t$-WL compensate for the smaller
displacement distance as can be seen from Figure~\ref{fig:fast-slow-gamma}.
The update factors take approximately 10x longer to reach steady-state for
the slower translation scale. Because of this update behavior, these methods
do not suffer in converging to the correct density of states from having a
static parameter such as SAMC $t_0$ in the update factor. SAMC methods that converge
in a reasonable amount of moves in Figure~\ref{fig:fast-slow-gamma}a do not 
necessarily do so for Figure~\ref{fig:fast-slow-gamma}b.


Sad performs similarly regardless of the displacement distance chosen although
it transitions to $1/\sqrt{t}$ convergence after 

\begin{figure}
\includegraphics[width=\columnwidth]{figs/s000/periodic-ww1_50-ff0_17-N256-entropy-error-default.pdf}
  \caption{
  The average entropy error for each MC method for $N=256$,
               $\delta_0 = 0.05\sigma$, $\eta = 0.17$, and $T_{\min} = 1$
               as a function of number of iterations run.  The error is
               averaged over 8 independent simulations, and the best
               and worst simulations for each method are shown as a
               semi-transparent shaded area.  The update factor for this
               system is in Fig.~\ref{fig:gamma-vs-t} above.}\label{fig:n256}
\end{figure}
%\begin{figure}
%  \includegraphics[width=\columnwidth]{figs/N256-lndos-comparison}
%  \caption{\davidsays{This could be prettier, and do we need it? We
%      definitely need a caption if we keep it.} Use this to create
%    description of the system.}\label{fig:n256-lndos-comparison}
%\end{figure}


\subsection{A periodic system with 256 atoms}
For this simulation, we chose a minimum reduced
temperature of $1.0$, which corresponds to an interesting energy range
from $-915$ to $-509$.  The number of important energy states for this
system is then $N_S = E_{\min} - E_{\max} = 406$.
The entropy of this system is has an entropy that
differs by 275, corresponding to a ratio of
$10^{119}$ between the highest and lowest density of states.

Figure~\ref{fig:n256} shows the average error in the
entropy as a function of moves for this system with the reasonable
displacement distance of $\delta_0 = 0.05\sigma$.  The solid lines
represent the average of the absolute value of the error in the
entropy averaged over eight simulations using different random number
seeds.  The range of average errors for each simulation is shown as a
shaded region around its mean error.  By the time $10^8$ moves have
been made all but the SAMC simulation with the shortest $t_0$ have
begun to converge as $1/\sqrt{t}$.  We then see the WL error begin to
saturate around $10^{10}$ moves.


For the larger system, SAMC parameter values of $t_0$ that gave rapid
convergence for the $N = 50$ system with $\delta = 0.05\sigma$ do not
converge rapidly.  Larger $t_0$ parameters are simulated for
comparison with SAD.  SAD has significantly smaller fluctuations
in the average entropy error for a given number of moves.

%{\color{red}
%Figure~\ref{fig:n256-lndos-comparison} shows the calculated entropy for
%the $N = 256$ system. The grey dash lines indicate the $S_{\max} = 509$
%point and the $S_{\min} = 915$. Only the reference method is shown.
%}

\subsubsection{Discussion on convergence and update factor}

A close examination of Fig.~\ref{fig:gamma-vs-t} shows that $\gamma$
for the Wang-Landau algorithm decreases much more rapidly than
$\nicefrac{1}{\sqrt{t}}$. It can be observed  for the simulations that
were carried out that all the methods from the weight-based family
struggle to converge if the update factor decreases too rapidlly.
Fig.~\ref{fig:N50-ff0.3-avg-error-slow} shows SAMC with inappropriate choices
of $t_0$ i.e. less than $\nicefrac{10^5}{\sqrt{t}}$ failing to converge
in a reasonable amount of time. Thus, the update factor $\gamma$, shown
as a function of simulation moves in Fig.~\ref{fig:gamma-vs-t}, proves
useful in determining which methods will converge in a reasonable
amount of time. We compare each method to a reference SAMC
simulation for an ideal choice of $t_0$.  This gives an unbiased
comparison with our developed method SAD. The dynamic version of SAMC
which we call SAD performs as well as SAMC (assuming the user can
correctly identify an appropriate choice of $t_0$ for SAMC).  SAD
outperforms WL and 1/t-WL? for the same given number of moves. The
absence of user defined parameters makes SAD an exciting alternative to
methods where user experience or successive implementation iteration
are needed to achieve reasonable results.

\section{Conclusions}

We have introduced a new algorithm SAD (Stochastic Approximation with a
Dynamic $\gamma$) that effectively samples the entire energy space for
a variety of system sizes.  Also, SAD proves effective in comparison
with other Monte-Carlo methods at converging to the correct density of
states.  The ease of implementation and the absence of user-defined
parameters makes SAD an ideal algorithm. SAD converges as quickly as
SAMC (in the case of an ideally chosen $t_0$ for our simulations. SAD
doesn't suffer from the weakness of SAMC, i.e. the user choosing
parameters that could lead to the simulation not converging in a
reasonable amount of time.

SAD decreases $\gamma$ in a controlled way that prevents the algorithm
from taking too long to converge (as in the case of SAMC with a poor choice
of $t_0$) or failing to converge such as is the case with WL for complex
systems. A major advantage is that the user does not need to determine a
priori how quickly the update factor $\gamma$ should be decreased as the
method SAD dynamically determines this.

We note that SAD should be tested on additional systems, and the
convergence of the method should be examined.  Also, a rigorous
mathematical justification for our choice of the update factor is
necessary to ensure the most optimal convergence of the algorithm.

\section{Acknowledgement}

%We gratefully acknowledge D.W. Siderius and
%J.R. Errington for helpful communications regarding the implementation
%of WL-TMMC.

\bibliography{paper}% Produces the bibliography via BibTeX.

\end{document}
