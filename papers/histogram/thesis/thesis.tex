\documentclass[11pt]{article}

%%% standard header
\usepackage[margin=1in]{geometry} % one inch margins
\usepackage{fancyhdr} % easier header and footer management
\pagestyle{fancyplain} % page formatting style
\usepackage{hyperref} % for linking references
\newcommand{\psl}{6pt} % store spacing as command for consistency
\setlength{\parindent}{0cm} % don't indent new paragraphs...
\parskip \psl % ... place a space between paragraphs instead
\usepackage[inline]{enumitem} % include for \setlist{}, use below
\frenchspacing % add a single space after a period
\usepackage{lastpage} % for referencing last page
\cfoot{\thepage~of \pageref{LastPage}} % "x of y" page labeling

%%% physics
\usepackage[boldvectors,braket]{physymb} % physics package
\newcommand{\bk}{\Braket} % shorthand for braket notation

%%% math
\usepackage{amsmath} % math package
\renewcommand{\t}{\text} % for text in math environment
\newcommand{\f}[2]{\dfrac{#1}{#2}} % shorthand for fractions
\newcommand{\p}[1]{\left(#1\right)} % parenthesis
\renewcommand{\sp}[1]{\left[#1\right]} % square parenthesis
\renewcommand{\set}[1]{\left\{#1\right\}} % curly parenthesis

%%% figures
\usepackage{graphicx,float,subcaption} % floats, etc.
\usepackage{footnote} % for footnotes in floats
\usepackage[font=small,labelfont=bf]{caption} % caption text options

%%% bibliography, table of contents
\usepackage[sort&compress,numbers]{natbib} % bibliography options
\bibliographystyle{apsrev4-1}
\usepackage[english]{babel} % allow editing bibliography title
\addto\captionsenglish{ % set bibliography title
  \renewcommand{\contentsname}{Table of Contents}
  \renewcommand\refname{\bf References}}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

%%% algorithm float
\floatstyle{plaintop}
\newfloat{algorithm}{thb}{lop}
\floatname{algorithm}{Algorithm}
\newenvironment{alg}
{\hrulefill\begin{enumerate}}
{\end{enumerate}\hrulefill}

\renewcommand{\title}{Optimizing Monte Carlo state space exploration
  of the square-well fluid}
\renewcommand{\author}{Michael A. Perlin}
\newcommand{\institution}{Oregon State University}
\newcommand{\department}{Department of Physics}
\newcommand{\supervisor}{Dr. David Roundy}
\renewcommand{\date}{21 April 2015}

%%% notes, etc.
\usepackage{color}
\newcommand{\red}[1]{{\bf \color{red} #1}}
\newcommand{\fixme}[1]{[\red{fixme:} \emph{#1}]}

\fancypagestyle{abstract}{
  \rhead{}
  \lhead{}
}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}

  \newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

  \center

  \textsc{\LARGE \institution}\\[1.5cm]
  \textsc{\Large \department}\\[0.5cm]

  \HRule \\[0.4cm]
  { \huge \bfseries \title}\\[0.4cm]
  \HRule \\[1.5cm]

  \begin{minipage}{0.4\textwidth}
    \begin{flushleft} \large
      \emph{Author:}\\
      \author
    \end{flushleft}
  \end{minipage}
  ~
  \begin{minipage}{0.4\textwidth}
    \begin{flushright} \large
      \emph{Supervisor:} \\
      \supervisor
    \end{flushright}
  \end{minipage}\\[4cm]

  {\large\date}

  \vfill

\end{titlepage}

\thispagestyle{empty}

\newpage

\tableofcontents

\thispagestyle{empty}

\newpage

\listof{figure}{\Large List of Figures}

\listof{algorithm}{\Large List of Algorithms}

\thispagestyle{empty}

\newpage

\setcounter{page}{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Abstract}
\label{sec:abstract}

\thispagestyle{abstract}

We wish to identify and develop efficient numerical methods for
determining thermodynamic properties of the square-well fluid in order
to test square-well density functional theories. The liquid-vapor
phase interface is an interesting regime for testing density
functional theories, but unbiased Monte Carlo simulations are
incapable of sampling the low energy fluid states which dominate the
partition function at low (i.e. liquid-state) temperatures. Several
generic Monte Carlo ``histogram methods'' for collecting statistics on
low energy system states have been developed \fixme{by whom?}, but
little or no literature exists on their systematic comparison and
their application to the square-well fluid. The square-well fluid in
particular introduces application challenges not manifest in
traditional models for testing and benchmarking such numerical
techniques (e.g. the Ising model). We propose to implement these
methods in Monte Carlo simulations of the square-well fluid, determine
appropriate performance metrics for their comparison, and identify
those methods which are most efficient and effective at sampling
otherwise improbable system states.

\fixme{Rewrite after the rest of the thesis is completed}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

\fixme{talk more about phase transitions and the critical point?}

Understanding the behavior of fluids near the critical point, or the
point at which the distinction between liquid and gas ceases to become
well-defined, is a major challenge in the field of classical density
functional theory. Studying the critical point of a fluid via
simulations requires exploring the boundary between its liquid and
gaseous phases. The square-well fluid (Section \ref{sec:sw_fluid}) is
the simplest system with a liquid-vapor phase transition, and is
therefore of interest for studying the critical point of fluids. For a
generic square-well fluid to be observed in a liquid state, it must
have a localized distribution of the spheres which make up the
fluid. Such a distribution implies a low energy fluid state, as all
interactions between spheres are attractive. Conversely, a low energy
liquid state of a square-well fluid otherwise observable in both
liquid and gaseous phases implies a localized distribution of
spheres. The search for liquid states of the square-well fluid is thus
equivalent to the search for low energy states.

Monte Carlo simulations (Section \ref{sec:mc_sim}) are a standard
method for studying the thermodynamic properties of a system. In the
most straightforward implementation of Monte Carlo simulations,
however, highly localized distributions of spheres are extremely
unlikely to occur, making such simulations impractical for studying
the critical point of the square-well fluid. Several generic
computational methods, called histogram methods (Section
\ref{sec:histogram_methods}), exist for dealing with such a problem;
namely, that interesting regions of a system's state space cannot be
sampled via standard Monte Carlo simulations in a reasonable amount of
time. There is no consensus, however, on the relative efficacy of
these methods. Furthermore, some methods, such as the Wang-Landau
method\cite{wang_landau}, require tuning several free parameters that
lack both ``obvious'' values and systematic means for assigning
them. This work will thus attempt to quantify and compare the
effectiveness of a few such methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
\label{sec:background}

\subsection{The square-well fluid}
\label{sec:sw_fluid}

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.47\textwidth]{figs/square-well.pdf}
  \caption[The square-well pair potential]{The square-well pair
    potential can be used to model short range forces to first
    order. The infinite potential at $r<\sigma$ simply enforces the
    condition that spheres cannot overlap. At $r>\lambda\sigma$, the
    potential is null, and makes no contribution to the fluid's
    internal energy.}
  \label{fig:pair_potential}
\end{figure}

The square-well (SW) fluid is a simple model used in classical density
functional theories to capture low order effects of short-range
attractive forces, such as the van der Waals force. The fluid is
composed of spheres with diameter $\sigma$ which have a pair potential
\begin{align}
  v_{sw}\p{\vec r}=\left\{
    \begin{array}{ll}
      \infty & \abs{\vec r}<\sigma \\
      -\epsilon & \sigma<\abs{\vec r}<\lambda\sigma \\
      0 & \abs{\vec r}>\lambda\sigma
    \end{array}
  \right., \label{eq:pair_potential}
\end{align}
also shown graphically in Figure \ref{fig:pair_potential}, where the
parameters $\lambda$ and $\epsilon$ are referred to as the well width
and depth, respectively. The first ($\abs{\vec r}<\sigma$) part of
this potential forbids spheres from overlapping, whereas the second
($\sigma<\abs{\vec r}<\lambda\sigma$) associates an energy $-\epsilon$
with each pair of spheres whose centers are within distance of
$\lambda\sigma$ of each other (where typically, $\lambda\in(1,3]$).
The net potential energy of the square-well fluid is thus
\begin{align}
  E=\sum_{i<j}v_{sw}\p{\vec r_i-\vec r_j},
  \label{eq:internal_energy}
\end{align}
where $\vec r_i$ is the position of the $i$-th sphere. As the
potential energy $E$ is the primary form of energy concerning us in
this paper, we will refer to $E$ as simply the ``energy'' of the
fluid. An important feature of the square-well fluid is that its
energy is always an integer multiple of the well depth. A homogeneous
square-well fluid is thus uniquely identified by its well width
$\lambda$ and filling fraction $\eta$ (i.e. the proportion of space
filled by spheres; a dimensionless density), as all other properties
can be normalized to the natural energy scale $\epsilon$ and length
scale $\sigma$. In practice, our simulation codes use dimensionless
energies $E/\epsilon$, temperatures $kT/\epsilon$, and distances
$r/\sigma$.

\subsection{Monte Carlo fluid simulations}
\label{sec:mc_sim}

While model systems are powerful tools for understanding complex
physical systems, they do not themselves exist in the real
world. Consequently, direct experimental tests of theories for model
system (e.g. square-well density functional theories) are not
possible. For this reason, model system theories are commonly tested
against Monte Carlo simulations. Proper implementation of Monte Carlo
methods to study completely characterized systems ensures that
statistical results from simulations converge on the exact properties
of the simulated system in the infinite simulation time
limit. Furthermore, uncertainties in quantities computed via Monte
Carlo simulations are typically well-defined, monotonically decreasing
functions of simulation time, allowing one to run simulations to the
desired level of accuracy.

\subsubsection{Implementation}
\label{sec:mc_implementation}

\begin{algorithm}[tb]
  \caption{Unbiased Monte Carlo fluid simulation}
  \label{alg:MC}
  \begin{alg}

  \item Construct an initial ``typical'' fluid configuration.

  \item Attempt to change the position of one sphere (in general, a
    single fluid ``atom'' or ``molecule'') to a random location,
    rejecting the change if it results in a forbidden fluid
    configuration (e.g. two or more spheres overlap) and accepting the
    change otherwise. Attempting to move a single sphere is referred
    to as a move. \label{alg:mc_move}

  \item Repeat step \ref{alg:mc_move} for every other sphere in the
    fluid. Attempting to move every sphere once is referred to as an
    iteration of the simulation.
    \label{alg:mc_iteration}

  \item Repeat step \ref{alg:mc_iteration} indefinitely, or until data
    of sufficient quality has been generated, periodically collecting
    and dumping statistics on fluid states (e.g. energy, pair
    distribution histograms, etc.) to data files.

  \end{alg}
\end{algorithm}

Algorithm \ref{alg:MC} provides a sketch of unbiased Monte Carlo fluid
simulations. Such an algorithm is ``unbiased'' in the sense that it
collects statistics (i.e. data) on all valid system configurations
with equal probability. Statistics whose collection time scales as
$\mathcal O\p{1}$ with system size, meaning that increasing the number
of spheres $N$ in the simulation does not affect collection time, can
be collected after every move (defined in the algorithm). Statistics
whose collection time scales as $\mathcal O\p{N}$, meaning that
doubling $N$ doubles collection time, can be collected after each
iteration. In general, collection with $\mathcal O\sp{\chi\p{N}}$ time
scaling should not occur more often than once every $\chi\p{N}$ moves
(where $\chi\p{N}$ may be an arbitrary function of only $N$ void of
constant prefactors, e.g. $N\log N$, or $2^N$). These collection rules
ensure that scaling up simulations does not cause them to
asymptotically spend all of their time collecting statistics, and no
time actually simulating the fluid. Collected statistics are used to
find thermodynamic properties of the simulated fluid.

In this work, we are concerned with simulating the \emph{homogeneous}
square-well fluid. To avoid edge effects resulting from fluid behavior
near a wall, we employ periodic boundary conditions. The use of a
finite cell with periodic boundary conditions suppresses all density
fluctuations on scales larger than the dimensions of the simulated
fluid cell, thereby introducing a source of error. Addressing and
sequestering this error, however, is outside the scope of this work,
and involves considering the limit of numerical results as the number
of spheres $N\to\infty$ (with a constant filling fraction).

\subsubsection{Computing observables}
\label{sec:computing_observables}

The sort of Monte Carlo simulations described above are only capable
of collecting statistics on functions of system microstates, and
finding correlations between these functions. For example, in
simulation one might periodically compute both the energy $E\p{s}$ and
some system property $X\p{s}$, both of which are determined by the
microstate $s$, in order to find the mean value of $X$ at any given
energy $E$, i.e. $\bk{X}_E$. In the real world, however, information
about a system's microstates, and by extension functions of
microstates (e.g. the energy $E$), is inaccessible. One therefore
cannot measure $\bk{X}_E$ directly. Instead, one typically measures
the dependence of thermodynamic properties on macroscopic state
variables such as temperature, i.e. $\bk{X}_T$.

To find $\bk{X}_T$, we first need to understand the concept of a
density of states. Given an arbitrary system property $Y\p{s}$
(e.g. energy) determined by the microstate $s$, one can define a
density of states $D\p{Y}$ such that
\begin{enumerate*}[label=\roman*)]
\item $D\p{Y_0}$ is proportional to the number of microstates $s$ for
  which $Y\p{s}=Y_0$, and
\item $\sum_YD\p{Y}=1$.
\end{enumerate*}
The temperature dependence of $\bk{X}_T$ can be expressed in terms of
the expectation value $\bk{X}_E$ and the density of states $D\p{E}$ by
\begin{align}
  \bk{X}_T=\f1{Z\p{T}}\sum_E\bk{X}_ED\p{E}e^{-E/kT},
  \label{eq:XT_norm}
\end{align}
where the partition function $Z\p{T}$ is simply a normalization
factor, given by
\begin{align}
  Z\p{T}=\sum_ED\p{E}e^{-E/kT}.
\end{align}
To reduce redundant computations and numerical error in
implementation, we will use a partition function with an unnormalized
density of states,
\begin{align}
  \tilde Z\p{T}=\sum_E\tilde D\p{E}e^{-E/kT},
\end{align}
in terms of which
\begin{align}
  \bk{X}_T=\f1{\tilde Z\p{T}}\sum_E\bk{X}_E\tilde D\p{E}e^{-E/kT}.
  \label{eq:XT}
\end{align}

\subsection{Histogram methods}
\label{sec:histogram_methods}

Due to the fact that unbiased Monte Carlo simulations sample all of
state space randomly and without preference, a histogram $H\p{X}$ of
observations of some system property $X$ is directly proportional to
the density of states $D\p{X}$; that is, the number of observations
$H\p{X_0}$ of the system with $X\p{s}=X_0$ is proportional to the
total number of states $s$ for which $X\p{s}=X_0$. It is sometimes the
case, however, that the density of states in some region $R$ in the
range of possible $X$ is so low that it is practically impossible (via
unbiased Monte Carlo) to sufficiently sample $R$, that is, accumulate
a statistically significant histogram $H\p{X\in R}$, in any reasonable
amount of time.

\subsubsection{Biased sampling}
\label{sec:biased_sampling}

Histogram methods provide a means to address unbiased Monte Carlo's
inability to sufficiently sample regions of parameter space with a low
density of states by introducing a bias into the otherwise random
sampling of state space. A weighting function $w$ is introduced, whose
domain is the value of some property $X\p{s}$ which depends on the
system state $s$. An additional condition is then added to step
\ref{alg:mc_move} of Algorithm \ref{alg:MC} in order to accept an
attempted move: the weights $w\sp{X\p{s_i}}$ and $w\sp{X\p{s_f}}$ of
the initial (pre-move) state $s_i$ and final (post-move) state $s_f$
are used to determine the probability $P\p{s_i\to s_f}$ of accepting
an otherwise valid move via
\begin{align}
  P\p{s_i\to s_f}=\min\set{\f{w\sp{X\p{s_f}}}{w\sp{X\p{s_i}}},1}.
  \label{eq:move_prob}
\end{align}
This formula means that when $w\sp{X\p{s_f}}>w\sp{X\p{s_i}}$, the move
$s_i\to s_f$ is accepted; when $w\sp{X\p{s_f}}<w\sp{X\p{s_i}}$, the
ratio of these weights determines the probability of accepting the
move. Due to the fact that only ratios of weights determine
$P\p{s_i\to s_f}$, the weights $w\sp{X\p{s}}$ are scale-invariant,
meaning that their effect on simulations is unchanged by scale factors
that are constant with respect to $X\p{s}$.

\begin{algorithm}[tb]
  \caption{Biased Monte Carlo fluid simulation}
  \label{alg:biased_MC}
  \begin{alg}

  \item Construct an appropriate weight function $w\sp{X\p{s}}$, whose
    argument is a system property $X\p{s}$ determined by the
    microstate $s$.

  \item Construct an initial ``typical'' fluid configuration.

  \item Attempt to change the position of one sphere, rejecting the
    change if
    \begin{enumerate*}[label=\roman*)]
    \item it results in a forbidden fluid configuration, or
    \item a newly chosen random number on the interval $\sp{0,1}$ is
      larger than the probability given by (\ref{eq:move_prob}).
    \end{enumerate*}
    \label{alg:biased_mc_move}

  \item Repeat step \ref{alg:biased_mc_move} for every other sphere in
    the fluid.
    \label{alg:biased_mc_iteration}

  \item Repeat step \ref{alg:biased_mc_iteration} until data of
    sufficient quality has been generated.

  \end{alg}
\end{algorithm}

Employing biased Monte Carlo simulations, sketched out in Algorithm
\ref{alg:biased_MC}, allows one to construct weight functions that
favor some region of state space over others, as transitions to states
with higher weights are always accepted, whereas transitions to states
with lower weights may be rejected, artificially preventing the
simulated system from leaving interesting regions of state
space. Crucially, the bias introduced by weights can be reversed when
computing system properties from sampling statistics, as the weight of
a particular state is directly proportional to the probability bias of
that state; that is, a state with a weight of 2 will be sampled twice
as often as it would have been with a weight of 1. If we wish to
convert a histogram $H\p{X}$ of observations in a biased Monte Carlo
simulation into a numerical unnormalized density of states $\tilde
D\p{X}$ in $X$, we therefore divide the histogram by the corresponding
weights $w\p{X}$, i.e.
\begin{align}
  \tilde D\p{X}=\f{H\p{X}}{w\p{X}}.
  \label{eq:dos}
\end{align}
The normalized density of states $D\p{X}$ in $X$ is then
\begin{align}
  D\p{X}=\f{\tilde D\p{X}}{\sum_X\tilde D\p{X}}
  =\f{H\p{X}/w\p{X}}{\sum_XH\p{X}/w\p{X}}.
  \label{eq:dos_norm}
\end{align}
In general, the domain and shape of the weight function will depend on
the desired yields (e.g.  density of states, heat capacity) of a
simulation. A ``histogram method'' is an algorithm for determining a
weight function appropriate for a particular simulation.

In this paper, we will consider weights $w\p{E}$ which depend only on
the energy $E$ of the system. Due to the fact that the square-well
fluid can only have discrete energies $E=-n\epsilon$, where $n$ is a
non-negative integer and $\epsilon$ is the well depth, in simulation
we store the weight function as an array of values. We may therefore
use the terms ``weight function,'' and ``weight array,'' and
``weights'' interchangeably.

\subsubsection{Canonical weights}
\label{sec:canonical_weights}

The most common weight array $w\p{E}$ used by physicists for what is
called a canonical Monte Carlo simulation involves choosing a
particular temperature $T_0$, and using weights proportional to the
Boltzmann factor at that temperature, i.e.
\begin{align}
  w\p{E}=e^{-E/kT_0},
\end{align}
where there is no reason to normalize $w\p{E}$ due to the fact that
only ratios of weights, as per (\ref{eq:move_prob}), are ever used in
simulation. The fact that only ratios of weights are used in
simulation makes $w\p{E}$ scale-invariant.

The partition function at $T=T_0$ for simulations with canonical
weights is
\begin{align}
  \tilde Z\p{T_0}=\sum_E\tilde D\p{E}e^{-E/kT_0}
  =\sum_E\f{H\p{E}}{w\p{E}}~e^{-E/kT_0}
  =\sum_E\f{H\p{E}}{e^{-E/kT_0}}~e^{-E/kT_0}=\sum_EH\p{E},
  \label{Z_canonical}
\end{align}
and the value of a thermodynamic property $X\p{T_0}$
\begin{align}
  X\p{T_0}=\f1{\tilde Z\p{T_0}}\sum_E\bk{X}_E\tilde D\p{E}e^{-E/kT_0}
  =\f{\sum_E\bk{X}_EH\p{E}}{\sum_EH\p{E}}.
  \label{eq:X_canonical}
\end{align}
The simplifications in in (\ref{Z_canonical}) and
(\ref{eq:X_canonical}), which have no explicit dependence on $T_0$,
occur because canonical Monte Carlo simulations sample energies in
proportion to the distribution (over energy) of microstates at a
temperature of $T_0$.  Canonical Monte Carlo simulations can therefore
fail to sufficiently sample energies which are important
(i.e. energies with a non-negligible state probability density) at
different temperatures. As a consequence, such simulations should not
be used to determine properties $X\p{T\ne T_0}$, and are thus referred
to as ``fixed temperature'' simulations.

Though canonical Monte Carlo is simple to implement, its inability to
investigate a system at more than one temperature at a time is a
disadvantage for determining the temperature dependence of system
properties. In order to find the behavior of $\bk{X}_T$, one must run
many simulations at discrete temperature intervals; each such
simulation will yield one sample (e.g. one data point) of $\bk{X}_T$.

Sampling low energies, however, or understanding system behavior at
low temperatures, is even more problematic with canonical
weights. Using low temperature canonical weights will indeed force a
simulated system down to low energies, but will also likely freeze the
system into a local minimum of its energy landscape. Freezing into a
state means that a simulation will sample only a small portion of the
energy landscape, even though there may (and generally will) be many
other states with the same energy.

Due to these problems, we will not use canonical weights alone to
study the square-well fluid. We will, however, use canonical weights
for part of all weight arrays, as discussed in Section
\ref{sec:min_energy}.

\subsubsection{Broad energy sampling}
\label{sec:broad_energy_sampling}

Given the expression for $\bk{X}_T$ in (\ref{eq:XT}), sufficient
accumulation of statistics on $\bk{X}_E$ at all available energies in
principle allows one to determine $\bk{X}_T$ for any temperature
$T$. In practice, the density of states can fall off so quickly with
energy that some range of allowable energies is practically
inaccessible via Monte Carlo simulations, biased or otherwise. In such
a case, computing $\bk{X}_T$ to a reasonable degree of accuracy
requires sufficiently sampling the energies at which $\tilde
D\p{E}e^{-E/kT}$ dominates the sum in (\ref{eq:XT}). Section
\ref{sec:energy_range} provides more discussion on this subject,
namely on identifying the energies which are important for computing
system properties at a given temperature.

We will employ histogram methods in order to sample as broad of an
energy range as possible in each simulation. Broad energy sampling
will allow us to determine, for various square-well fluids,
\begin{enumerate*}[label=\roman*)]
\item the density of states $D\p{E}$, and
\item the temperature dependence $\bk{X}_T$ of various thermodynamic
  properties $X$, particularly at temperatures near the liquid-gas
  phase boundary.
\end{enumerate*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}
\label{sec:methods}

\fixme{Give an overview of the methods section}

\subsection{The flat histogram (multi-canonical) method}
\label{sec:flat_historam}

The flat histogram method, also called the multi-canonical method,
assumes complete knowledge of the density of states $D\p{E}$ of the
system in question, and solves (\ref{eq:dos}) for a weight array
$w\p{E}$ which should yield a flat energy histogram $H\p E=H_0$ to get
\begin{align}
  w\p E=\f1{D\p E},
  \label{eq:flat_weights}
\end{align}
where we may neglect the constant scale factor $H_0$ and do not worry
about normalization factor distinguishing $D\p{E}$ from $\tilde
D\p{E}$ due to the scale-invariance of $w\p{E}$.

For all but the most trivial or well-studied systems, however, the
density of states $D\p{E}$ is not known prior to simulation, and is in
fact one of the system properties which a simulation is intended to
determine. A naive way to implement the flat histogram method without
knowing a priori the density of states $D\p{E}$ involves first running
a simulation without weights (or with constant weights $w\p{E}=w_0$)
while collecting a histogram $H\p{E}$ of the observed energies
$E$. After some time (loosely defined), the histogram $H\p{E}$ may be
taken as an approximation of the density of states $\tilde D\p{E}$
(see Section \ref{sec:histogram_methods}), and used via
(\ref{eq:flat_weights}) to generate a weight array for an actual
simulation.

The problem with this implementation is that histogram methods are
generally necessary when some range of energies $R$ is inaccessible
via unbiased Monte Carlo simulations. In such a case, the energy
histogram $H\p{E\in R}$ after simulating for any reasonable amount of
time will be statistically insignificant or null, invalidating the
approximation $H\p{E}\approx\tilde D\p{E}$. This method will therefore
only ``flatten'' the histogram at energies $E$ which have been
sufficiently sampled for $H\p{E}$ to be statistically significant. The
end goal in implementing histogram methods, however, is precisely to
sample those energies that cannot be sufficiently sampled without
employing clever schemes to bias Monte Carlo simulations. The flat
histogram method therefore does not directly help us study the
square-well fluid. Nonetheless, the this method has pedagogical value
by enabling discussion of an ``optimal'' weight array for generating a
flat energy histogram in simulations. All algorithms to construct
$w\p{E}$ with the aim of producing a flat energy histogram should
converge on the same weight array, given by (\ref{eq:flat_weights}).

\subsection{The simple flat method}
\label{sec:simple_flat}



\fixme{Explain, provide algorithm. This method is our own.}

\begin{algorithm}[H]
  \caption{Simple flat initialization}
  \label{alg:simple_flat}
\end{algorithm}

\subsection{The Wang-Landau (WL) method}
\label{sec:wang_landau}

\fixme{Explain the motivation behind the Wang-Landau method, and
  provide the algorithm. Talk about the free parameters and roughly
  what they control.}

\begin{algorithm}[H]
  \caption{Wang-Landau calculation of weights}
  \label{alg:wang_landau}
\end{algorithm}

% \subsubsection{Modifications}
% \label{sec:wang_landau_mod}

% \fixme{Talk about our modifications to Wang-Landau made to adapt it
% to the square-well fluid, as well as (if I have time) other research
% groups' modifications.}

\subsection{The optimized ensemble (OE) method}
\label{sec:optimized_ensemble}

\fixme{Explain, provide algorithm.}

\begin{algorithm}[H]
  \caption{Finding weights by optimizing energy walker currents}
  \label{alg:optimized_ensemble}
\end{algorithm}
uu
\subsection{The transition matrix Monte Carlo (TMMC) method}
\label{sec:tmmc}

\fixme{Explain, provide algorithm. This method is sort of our own, but
  based on other work.}

\begin{algorithm}[H]
  \caption{Transition matrix Monte Carlo simulation}
  \label{alg:tmmc}
\end{algorithm}


\subsection{The hybrid OE-TMMC method}
\label{sec:oetmmc}

\fixme{Explain, provide algorithm. This method is sort of our own, but
  based on other work.}

\begin{algorithm}[H]
  \caption{Hybrid OE-TMMC simulation}
  \label{alg:oetmmc}
\end{algorithm}


\subsection{Identifying the energy range of interest}
\label{sec:energy_range}

Histogram methods typically rely on knowledge of the minimum and
maximum energies of the system in question \fixme{how?}, which are
unknown for the square-well fluid. Worse still, some square-well fluid
energies have such an incredibly low density of states that one cannot
reasonably expect to ever observe them via Monte Carlo, biased or
otherwise. As such, we need to identify an energy range of interest,
bounded by minimum and maximum ``important'' energies. \fixme{Rewrite,
  expand}

\subsubsection{State of maximal entropy}
\label{sec:max_entropy}

\fixme{The energy at which the density of states is maximal, i.e. the
  state of maximal entropy, sets an upper bound for the energy range
  in which appropriate weights are not obvious. Explain why.}

\subsubsection{Minimum important energy}
\label{sec:min_energy}

\fixme{Determining the minimum important energy is considerably
  trickier. Punchline: we have to choose a minimum temperature of
  interest, $T_{\t{min}}$, and the minimum important energy is the
  energy at which the slope of the density of states is
  $\exp\p{kT_{\t{min}}/\epsilon}/\epsilon$. This condition for the
  minimum important energy is directly related to canonical weights at
  a temperature of $T_{\t{min}}$.}

\subsubsection{Computing the density of states}
\label{sec:dos}

\fixme{This section might just become a part of the section on the
  minimum important energy. (\ref{eq:density_of_states}) gives us an
  explicit formula for the density of states if we are confident in
  our energy histogram. We can also compute the density of states from
  the transition matrix, which is sometimes more appropriate to do.}

% \subsection{End conditions}
% \label{sec:end_conditions}

% \fixme{Talk about how we want method-independent end conditions, if
% possible. Introduce the concept of statistically independent energy
% samples, and explain two measures (one optimistic, and one
% pessimistic) for determining the number of statistically independent
% times we have samples a given energy.}

% \fixme{Talk about two end conditions: a fixed number of times which
% we require to independently sample the minimum important energy, and
% an enforced fractional error in the number of times we have
% independently sampled any energy (at a given minimum temperature).}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}
\label{sec:results}

\subsection{Implementation and overview}
\label{sec:implementation}

\fixme{Talk about the relative difficulty of implementing each method,
  as well as the relative difficulty of optimizing or tuning each
  method. Free parameters are generally bad, and especially so if they
  come without ``obvious'' or prescribed values. Justify our
  modifications to Wang-Landau by showing that we did not make it
  worse. Include mention of how memory requirements for transition
  matrix Monte Carlo scale poorly with system size, as well as energy
  resolution for systems with continuous energies.}

\subsection{Simulation results and errors}
\label{sec:results_and_errors}

\fixme{Talk about the energy histogram, weights, and density of states
  yielded by the methods. These should all be fairly similar. Talk
  about why this is so.}

\begin{figure}[H]
  \centering
  \caption[Energy histograms]{Energy histograms}
  \label{fig:histograms}
\end{figure}

\begin{figure}[H]
  \centering
  \caption[Energy weights]{Energy weights}
  \label{fig:weights}
\end{figure}

\begin{figure}[H]
  \centering
  \caption[Normalized densities of states]{Normalized densities of
    states}
  \label{fig:density_of_states}
\end{figure}

\fixme{Talk about initialization times, and how they scale with system
  size}

\begin{figure}[H]
  \centering
  \caption[Initialization iterations vs. system size]{Initialization
    iterations vs. system size}
  \label{fig:scaling}
\end{figure}

\fixme{Discuss energy initialization times, sampling rates, and errors
  in computed thermodynamic properties. Explain that errors are given
  relative to a simulation which ran much longer, and talk about how
  we ran that simulation (which method did we use?). Show that all
  methods converge on the same answer as they simulate for longer.}

\begin{figure}[H]
  \centering
  \caption[Optimistic energy sampling rates]{Optimistic energy
    sampling rates}
  \label{fig:opt_sample_rate}
\end{figure}

\begin{figure}[H]
  \centering
  \caption[Pessimistic energy sampling rates]{Pessimistic energy
    sampling rates}
  \label{fig:pes_sample_rate}
\end{figure}

\begin{figure}[H]
  \centering
  \caption[Initialization iterations vs. data quality]{Initialization
    iterations vs. data quality}
  \label{fig:quality}
\end{figure}

\begin{figure}[H]
  \centering
  \caption[Specific internal energy]{Specific internal energy}
  \label{fig:internal_energy}
\end{figure}

\begin{figure}[H]
  \centering
  \caption[Specific heat capacity]{Specific heat capacity}
  \label{fig:heat_capacity}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusions}

\fixme{What have we learned? What recommendations can we make? When is
  it appropriate to use which histogram methods?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nocite{*} \bibliography{thesis}

\end{document}
