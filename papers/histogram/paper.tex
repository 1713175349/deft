\documentclass[letterpaper,twocolumn,amsmath,amssymb,pre,aps,10pt]{revtex4-1}
\usepackage{graphicx}% Include figure files
\usepackage{color}

\newcommand{\red}[1]{{\bf \color{red} #1}}
\newcommand{\green}[1]{{\bf \color{green} #1}}
\newcommand{\blue}[1]{{\bf \color{blue} #1}}
\newcommand{\cyan}[1]{{\bf \color{cyan} #1}}

\newcommand{\davidsays}[1]{{\color{red} [\green{David:} \emph{#1}]}}
\newcommand{\mpsays}[1]{{\color{red} [\blue{Michael:} \emph{#1}]}}

\begin{document}
\title{Applying the optimized ensemble histogram method to the
  square-well liquid}

\author{Jordan K. Pommerenck} \author{Michael A. Perlin} 
\author{Tanner T. Simpson} \author{David J. Roundy}
\affiliation{Department of Physics, Oregon State University,
  Corvallis, OR 97331}

\begin{abstract}
  We have applied the clever histogram method to the square-well
  liquid.
\end{abstract}

\maketitle

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-E}
  \caption{A histogram plot demonstrating the difficulty of using
    canonical Monte Carlo to simulate a system.  A simulation at a
    give temperature only provides information fo a small number of
    energy states.\label{fig:histograms}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-weights}
  \caption{The weight functions from different methods.}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-dos}
  \caption{A density of states plot demonstrating the difficulty of
    using canonical Monte Carlo to simulate a system.  A simulation at
    a give temperature only provides information fo a small number of
    energy states.\label{fig:dos}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-sample-rate}
  \caption{Inverse sampling rates from different methods.}
\end{figure}

% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-scaling}
%   \caption{Scaling.\label{fig:scaling}}
% \end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-u}
  \caption{Specific internal energy.\label{fig:u}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-hc}
  \caption{Specific heat capacity.\label{fig:hc}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-S}
  \caption{Specific entropy.\label{fig:S}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-u_err}
  \caption{Specific internal energy.\label{fig:u}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-hc_err}
  \caption{Specific heat capacity.\label{fig:hc}}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-s_err}
  \caption{Error in specific entropy.\label{fig:Serr}}
\end{figure}

%% \begin{figure}
%%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-rdf}
%%   \caption{Radial distribution function.\label{fig:rdf}}
%% \end{figure}

As shown in Fig.~\ref{fig:histograms}, there are several difficulties
encountered when running Monte Carlo simulations at a fixed
temperature.  On the top plot, which shows three simulations run at
$\lambda = 1.3$, it is apparent that each given canonical simulation
only provides statistical information for a small number of energy
states.  The bottom plot shows an even more serious issue: when
running a simulation at a fixed temperature, it is possible to become
frozen in a state that is far from the ground state, as happened in
the $k_{B}T=0.1\epsilon$ simulation.

% \begin{table*}
% \input{figs/scaling-table-ww130-ff30}
% \caption{Scaling data}
% \end{table*}

\section{New ideas for algorithms}

\subsection{Finding maximum probability state}

Technically maximum histogram or maximum entropy.

Loop:

run iterations until we see $N$ energy changes, counting the number of
returns to the initial energy.  If there are sufficient returns, then
we are at the maximum, where sufficient returns will be determined by
a random walk (weighted?) where we compute the expected number of
returns if the walk is not weighted.  We may want to correct to get
all the way to the top.

If we are not at the top yet, we try again.  Eventually we will be.

\subsection{Finding variance of the histogram}

I assume we first ran the max-entropy/histogram algorithm, which gives
us a maximum value of $E_0$.  We then zero the histogram.

We run for a short while, and then compute the mean and standard
deviation of the energy from the histogram: $\bar E$ and $\sigma$.
While doing this, we track the number of returns to the initial state
$n$.  Now the uncertainty in the mean is given by
\begin{equation}
  \Delta \bar E \approx \frac{\sigma}{\sqrt{n}}
\end{equation}
if $\Delta \bar E \ll (E_0 - \bar E)$ then we can conclude that we are
certain where the maximum is, and how wide it is.

\begin{figure*}
  \includegraphics[width=0.33\textwidth]{figs/periodic-ww130-ff30-N20-tmmc-golden-transitions}\hfill%
\includegraphics[width=0.33\textwidth]{figs/periodic-ww130-ff30-N20-tmmc-transitions}\hfill%
\includegraphics[width=0.33\textwidth]{figs/periodic-ww130-ff30-N20-tmmc-golden-tmmc-compare-transitions}
  \caption{A plot showing the probability of energy transitions (with
    no weighting) from given energy states.\label{fig:transitions}}
\end{figure*}

\subsection{Tracking transitions}

We can track the number of attempted transitions from each energy
eestate to each other energy state.  This statistic has the advantage of
being approximately independent of the weighting function used
(although it is highly dependent on the step size).  Thus we could
continue to refine this statistic, even as we are updating the weight
array.  Here are two papers that seem highly relevant to this
method~\cite{wang1999transition, wang2002transition}, but I'm not sure
if they're identical.

The ratio of density of states at two energies should be proportional
to the ratio of upgoing and downgoing transition rates between those
two energies.  Thus these transitions give us information about
$\Delta$$\mathcal{D}$($\varepsilon$) (where $\mathcal{D}$($\varepsilon$) 
is the density of states).

Figure~\ref{fig:transitions} below shows the transition matrix for a
20-sphere system, computed during initialization using the Wang-Landau
algorithm.

How can we use this?

\subsubsection{Generating $w$ from the transition matrix}

We should be able to generate a set of weighting functions directly
from the transition matrix.  We could do this either by optimizing for
a flat histogram, or by optimizing the round-trip rate.  Both would be
interesting to implement.

One natural approach would use linear algebra.  If the normalized
transition matrix is $\mathcal{T}$, then the density of states is given by
\begin{equation}
  \mathcal{D}(\varepsilon) = \mathcal{T}\mathcal{D}(\varepsilon) 
\end{equation}
By solving this equation for $\mathcal{D}$($\varepsilon$), we could find a 
good approximation for $w$ to obtain a flat histogram.

I expect that we could use similar math to the optimized ensemble
folks to optimize the round-trip rate.

\subsubsection{Initializing using the transition matrix directly}

An alternative initialization algorithm involves using the transition
matrix directly during initialization.  This process was proposed by
Swendsen \emph{et al.} as a modification for their initialization
procedure~\cite{swendsen1999transition}.  In this case, the transition
matrix is continually updated, thus modifying the relative weighting
of different energies as the simulation proceeds.  This strictly
speaking no longer satisfies detailed balance, but as the simulation
proceeds this violation will quickly become increasingly small.

This raises the question of how to use the transition matrix to
directly determine transition probabilities, since the self-consistent
eigenvalue solution is unsuitable.  The key is to recognize that we do
not require weights for each energy but only \emph{differences} of
weights for on pair of energies at a time.  This is a much smaller
problem.

Swendsen \emph{et al.} find for a flat distribution, the acceptance
rate for a given transition is equal to the ratio of that element in
the transition matrix to the element corresponding to the reverse
transition~\cite{swendsen1999transition}.  Naturally, for a different
distribution, we can scale the acceptance ratios according to the
ratio of occupations desired.  We modify this algorithm to always
accept transitions to a lower energy, thus employing a broad histogram
method that does not much sample energies above the maximum-entropy
energy.

Swendsen only uses this approach after a slightly hokey two-stage
initialization.  It seems to work pretty well, and asymptotically it
does have detailed balance.

\newpage

%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-u_errors}
  %\caption{$u$ error scaling.}
  %\label{fig:scaling-u_err}
%\end{figure}
%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-cv_errors}
  %\caption{$c_V$ error scaling.}
  %\label{fig:scaling-cv_err}
%\end{figure}
%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-s_errors}
  %\caption{$s$ error scaling.}
  %\label{fig:scaling-s_err}
%\end{figure}

%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-u_error_comp}
  %\caption{$u$ error comparisons.}
  %\label{fig:scaling-u_err_comp}
%\end{figure}
%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-cv_error_comp}
  %\caption{$c_V$ error comparisons.}
  %\label{fig:scaling-cv_err_comp}
%\end{figure}
%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-s_error_comp}
  %\caption{$s$ error comparisons.}
  %\label{fig:scaling-s_err_comp}
%\end{figure}


% \subsection{Convergence tests}

% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-dos-conv-T10}
%   \caption{Convergence}
%   \label{fig:conv-dos}
% \end{figure}

% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-u-conv-T10}
%   \caption{Convergence}
%   \label{fig:conv-u}
% \end{figure}

% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-hc-conv-T10}
%   \caption{Convergence}
%   \label{fig:conv-hc}
% \end{figure}
% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-S-conv-T10}
%   \caption{Convergence}
%   \label{fig:conv-S}
% \end{figure}

\section{Introduction}

\subsection{Motivation and Background}

An accurate understanding of phase coexistence of liquid-vapor water-based systems is of increasing importance.  Chemical and biological molecular studies performed in water-based systems greatly benefit from an a priori knowledge of dynamics of the system.  Thus, considerable research has been done in recent years in developing models for the system and algorithms~\cite{wang1999transition, wang2002transition, swendsen1999transition, Broadhistogram, trebst2004optimizing, wessel2007optimized} for predicting phase coexistence from the thermodynamic properties of the system under study.  Popular models for the potential energy include:  the Double Gaussian-Core model~\cite{speranza2014phase, prestipino2014twofold}, the Gaussian-Core model~\cite{rane2013monte, krekelberg2009anomalous, mausbach2009solid}, the Soft-Sphere model~\cite{kreitzberg2015monte, torrie1977nonphysical}, and the Hard-Sphere model~\cite{hughes2013classical, lurie2014approach, krebs2014improved, schulte2015thesis, perlin2015thesis} for fluids.  While there exists a number of algorithms available for determining the thermodynamic properties of the system, among the most readily applied are molecular dynamics, density functional theory, and Monte-Carlo simulation.  In this work, we will discuss the Soft-sphere energy models and employ re-weighting histogram Monte-Carlo methods to solve for thermodynamic properties using the Soft-sphere model fluid.  
 
\subsection{Potential Energy Model}

The literature presents a number of ways to treat the system of interest.  Particle interactions via the system potential is defined in order to initialize the calculation.  Current work in the literature favors the following as system models for liquid-vapor systems: Double Gaussian-Core model, Gaussian-Core model fluid, Soft-sphere model, and Hard-sphere model fluid.  The Soft-sphere model or Weeks-Chandler-Anderson Pair Potential~\cite{andersen1971relationship, weeks1971role} can be written as
\begin{equation}
	\mathcal{V}_{\mathcal{WCA}}=\begin{cases}4\epsilon \bigg[\bigg(\frac{\sigma}{r}\bigg)^{12}-\bigg(\frac{\sigma}{r}\bigg)^{6}\bigg]+\epsilon & 0 < r<2^{1/6}\sigma\\0 & otherwise\end{cases} 
\end{equation}
where $\sigma$ represents the core separation of two spheres.  $r^{-12}$ is the reference potential while $r^{-6}$ is the system perturbation.  Two overlapping spheres experience this potential energy which is an extension of a Lennard-Jones potential with an offset $\epsilon$ such that the force experienced by the spheres is both attractive and repulsive~\cite{hoover1970soft}.  For low temperatures, the Soft-sphere potential essentially reduces to that of the Hard-sphere potential, since at low temperatures the liquid system now exhibits behavior closer to that of a solid.
Scale invariance plays an important role when using Monte-Carlo simulations to determine thermodynamic properties of the system. As a result of this scale invariance, it is possible to determine the thermodynamic properties at a single temperature and extrapolate to determine properties at any temperature.  In the 1970s, various thermodynamic properties were determined for the Soft-sphere potential using Metropolis Monte-Carlo~\cite{andersen1971relationship, hansen1970phase}; however, these properties were not calculated with the family of re-weighting Histogram methods introduced somewhat later.

\subsection{Histogram Monte-Carlo}

While Monte-Carlo simulations have been performed on systems modeled with a Soft-sphere potential~\cite{andersen1971relationship, hansen1970phase, sun2013efficient, ghoufi2016computer} since the 1970s, we incorporate several new ideas using re-weighting Histogram methods that have not been investigated.  A major motivation for using Histogram methods is that the algorithms give quantitative estimates for the density of states $\mathcal{D}$($\varepsilon$).  Once the density of states is known, it is possible to calculate free energies and heat capacities, as well as, other thermodynamic properties.  In this work, we will calculate the free energy of the system $\mathcal{F}$, the heat capacity $\mathcal{C}$, and the entropy $\mathcal{S}$.  For the simulations, a particular well-width $\lambda=1.3$, packing-fraction $\eta=0.22$, and number of particles $\mathcal{N}$.  In this work, we compare the following re-weighting Histogram methods:  Wang-Landau, Multi-Canonical (Simple-Flat), Transition Matrix Monte-Carlo (TMMC), and Optimized Ensemble Transition Matrix Monte-Carlo (OETMMC).  In the following section, we will discuss these techniques, as well as, our implementation scheme for the Histogram algorithms.

\section{Methods}

\subsection{Re-Weighting Histogram Algorithms}

\subsubsection{Wang-Landau Algorithm}

The Wang-Landau algorithm is a powerful re-weighting Histogram method developed and refined well into 21st century~\cite{wang2001determining}.  While methods such as Metropolis Sampling and Swendsen-Wang cluster flipping~\cite{swendsen1987nonuniversal} generate a narrow distribution and require sampling at individual temperatures, the Wang-Landau algorithm uses a flat histogram and performs a random walk in energy space to determine the density of states~\cite{LandauMinSampling}.  Wang-Landau's major tenant is that when counting the histogram $\mathcal{H}$($\varepsilon$), the energy occurences should form a flat distribution.  The criteria for flat sampling is given by
\begin{equation}
	\frac{\min_{\varepsilon} \mathcal{H}(\varepsilon)}
	{\big\langle\mathcal{H}(\varepsilon)\big\rangle } 
	> \gamma 
\end{equation}
where $\gamma$ is usually set between 0.75 and 0.99 and is determines how many
times each energy is sampled relative to the mean number of visits.  Difficulties
can occur with this flatness criteria due to the fact that some energies on a
given energy range might never be sampled~\cite{haber2014transition}.  The algorithm
is briefly outlined below  
\begin{equation}
	\ln{\mathcal{H}_{t+1}(\varepsilon,N)}=\ln{\mathcal{H}_{t}(\varepsilon,N)}
	+\ln{\mathcal{F}}
\end{equation}
\begin{equation}
	\ln{\mathcal{F}_{k+1}}=\frac{u}{2}\ln{\mathcal{F}_{k}}
\end{equation}
for $u$ typically greater than 1 and $\mathcal{F}$ is the modification factor for
the density of states.  We then accept a new configuration for the density of 
states with a probability given by the following
\begin{equation}
	\mathcal{P}(\varepsilon_i \rightarrow \varepsilon_j) 
	= \min[1,e^{\ln{\mathcal{H}(\varepsilon_i)}-\ln{\mathcal{H}(\varepsilon_j)}}]
\end{equation}
Zhou and Bhatt explored the convergence of the Wang-Landau algorithm and 
determined that choosing a large modification factor $\mathcal{F}_0 = e^{4}$ and
rapidly reducing the factor by 10 during each step resulted in reduced statistical 
error~\cite{zhou2005understanding}.

\subsubsection{The Multi-Canonical Algorithm}

The

\subsubsection{The TMMC Algorithm}

The Transition Matrix Monte-Carlo algorithm is often used in the family of broad histogram methods~\cite{swendsen1999transition}.  In fact, without the idea of a transition matrix detailed balance cannot be preserved for the Broad histogram method~\cite{wang1999broad}.  The sampling dynamics eventually produce a histogram that is flat when using the transition matrix.  The idea of the transition matrix was first developed in the 1970s by Kieth Hastings~\cite{hastings1970monte}.  We begin by first initializing a transition matrix $\mathcal{T}_{ij}=0$ for all energies.  We next define the transition matrix and attempt to make a defined move which is either accepted or rejected with an acceptance probability given by
\begin{equation}
\mathcal{T}_{ij} = \frac{\mathcal{T}_{d}(\varepsilon_j,-\Delta{\varepsilon})}
{\sum\limits_{\Delta{\varepsilon}}\mathcal{T}_{d}
(\varepsilon_j,\Delta{\varepsilon})}
\end{equation}
\begin{equation}
\begin{split}
\mathcal{P}(\varepsilon_i \rightarrow \varepsilon_j) = 
\max\bigg[\frac{\mathcal{T}_{d}(\varepsilon_i,\Delta{\varepsilon})
+\mathcal{C}}{\mathcal{T}_{d}(\varepsilon_f,-\Delta{\varepsilon})
+\mathcal{C}}\\\frac{\sum\limits_{\Delta{\varepsilon'}}\mathcal{T}_{d}
(\varepsilon_i,\Delta{\varepsilon})+\mathcal{C}} 
{\sum\limits_{\Delta{\varepsilon'}}\mathcal{T}_{d}(\varepsilon_f,
\Delta{\varepsilon})+\mathcal{C}},\exp{\bigg(-\frac{\Delta{\varepsilon}}
{k_{B}T}\bigg)}\bigg]
\end{split}
\end{equation}
The Multi-Canonical (Flat histogram methods) ensemble is designed in such a way so as to ensure visiting every energy level with equal probability~\cite{swendsen1999transition}.  This would hardly be ideal using a Broad histogram approach where the acceptance rate should be related to the sampled energy.  While similar to the Multi-Canonical approach in that both methods sample over all energy space for a given temperature, the Broad histogram methods have the advantage of being able to sample over a larger temperature scale and away from the critical point~\cite{Broadhistogram,BroadHistogram2}.  This feature can allow the methods to solve more accurately and computationally efficient then Flat histogram methods.

\subsubsection{The OETMMC Algorithm}

The Optimized Ensemble method has similar features to the TMMC method.  In  addition, it re-weights the histogram such that unlikely (not-probable) transitions are not equally favored.  This allows the simulation low-energy sampling rate to maximized.  We outline an approach to updating the weights $\mathcal{W}_{i}$ based on the density of states $\mathcal{D}(\varepsilon)$~\cite{trebst2004optimizing,wessel2007optimized}.  We begin the algorithm by starting with the weights proportional to the reciprocal of the density of states.  Next, enter a loop which has the purpose of calculating a fraction
$\chi(\eta)$ in terms of histograms $\mathcal{H}_{+}(\eta)$ and $\mathcal{H}_{-}(\eta)$ and updating the
global histogram $\mathcal{H}(\varepsilon)$.
\begin{equation}
	\chi = \frac{\mathcal{H}_{+}(\eta)}{\mathcal{H}_{+}(\eta)+\mathcal{H}_{-}(\eta)}
\end{equation}
We next update the statistical weights and calculate a new acceptance probability.
\begin{equation}
	\mathcal{W}_{i+1}(\eta) = \mathcal{W}_{i}(\eta)\sqrt{\frac{1}{\mathcal{H}_{+}(\eta)+
	\mathcal{H}_{-}(\eta)}\bigg|\frac{\text{d}\chi(\eta)}{\text{d}\eta}\bigg|} 
\end{equation}
Once the statistical weights are found the can relate the density of states and the 
global histogram. With the completetion of finding the global histogram, thermodynamic
properties can now be calculated from the density of states.

\subsection{New Ideas for Re-Weighting}

Our algorithm ideas

%\section{Introduction}

%\subsection{Motivation and Background}

%A greater understanding of thermodynamic properties of biological systems has been
%a driving impetus over the last decade.  For instance, different cancers can be 
%identified by their respective thermodynamic entropies which play a role in determining
%their individual signaling networks~\cite{rietman2016thermodynamic}.  Also, free 
%energy is an important thermodynamic property used to predict protein folding
%and ligand binding~\cite{perez2016advances}.  Algorithms that can predict these
%thermodynamic properties with an appropriate potential are essential in
%furthering our understanding of various biological systems.   

%There are primarily two main ways to calculate thermodynamic properties such as 
%free energy, the conditions for phase-coexistence, and critical 
%points~\cite{haber2014transition}.  The first method is molecular dynamics. 
%By using Newton's laws, it is posible to determine the dynamical evolution of a
%given thermodynamic system.
%The precursur to molecular dynamics, the second method is known as Monte-Carlo.  
%Although, somewhat older, it is no less powerfull than molecular dynamic theory.  
%Indeed, Monte-Carlo computation can often be much simpler to implement, as well 
%as, having the ability to be more versatile than its conterpart.

%Monte-Carlo simulations make use of random number generation to sample a generalized
%ensemble.  Monte-Carlo methods have found their niche among computational 
%algorithms through their ability to successfully treat large systems of interacting 
%particles~\cite{landau2014guide, Simple-Liquids}.  Histogram methods are a special
%class of Monte-Carlo techniques.  A major motivation for using Histogram methods 
%is that the algorithms give quantatative estimates for the density of states
%$\mathcal{D}$($\varepsilon$).  From the density of states, it is possible to calculate free 
%energies and heat capacities, as well as, other thermodynamic variables.  It is
%quite natural to treat a system under consideration as an idealized square-well 
%fluid~\cite{hughes2013classical, lurie2014approach,krebs2014improved}.  The potential benefits are that the model
%is able to accurately describe lower order effects due to short range attractive
%attractive forces~\cite{schulte2015thesis, perlin2015thesis}.  In this work, 
%we will compare various histogram methods by calculating thermodynamic properties 
%such as internal energy, heat capacity, and entropy.  By examining the relative 
%error when calculating these thermodynamic properties, a comparison can be made 
%among the presented histogram methods.


%\subsection{Comparison of Re-weighting Methods}

%We examine the Simple-Flat, Wang-Landau, Transition Matrix Monte-Carlo (TMMC),
%and Optemized Ensemble Transition Matrix Monte-Carlo (OETMMC) in this work.  The
%Simple-Flat and Wang-Landau are examples of flat histogram methods.  The TMMC and 
%OETMMC are in the family of broad histogram methods.  In fact, the TMMC method 
%reduces to the broad histogram method in the case of temperature going to 
%infinity~\cite{wang1999transition}.  In the following subsections, we will briefly
%outline and describe each of the presented methods.

%\subsubsection{The Simple-Flat (Multi-Canonical) Algorithm}

%We examine the Simple-Flat (I am working here!!!)

%\subsubsection{The Wang-Landau Algorithm}

%The Wang-Landau algorithm is in the family of flat histogram 
%methods~\cite{wang2001determining}.  While methods such as Metropolis Sampling and
%Swendsen-Wang cluster flipping~\cite{swendsen1987nonuniversal} generate a 
%narrow distribution and require sampling at individual temperatures, the Wang-
%Landau algorithm uses a flat histogram and performs a random walk in energy space to
%determine the density of states~\cite{LandauMinSampling}.  Wang-Landau's major tenant is that when 
%counting the histogram $\mathcal{H}$($\varepsilon$), the energy occurences should 
%form a flat distribution.  The criteria for flat sampling is given by
%\begin{equation}
	%\frac{\min_{\varepsilon} \mathcal{H}(\varepsilon)}
	%{\big\langle\mathcal{H}(\varepsilon)\big\rangle } 
	%> \gamma 
%\end{equation}
%where $\gamma$ is usually set between 0.75 and 0.99 and is determines how many
%times each energy is sampled relative to the mean number of visits.  Difficulties
%can occur with this flatness criteria due to the fact that some energies on a
%given energy range might never be sampled~\cite{haber2014transition}.  The algorithm
%is briefly outlined below  
%\begin{equation}
	%\ln{\mathcal{H}_{t+1}(\varepsilon,N)}=\ln{\mathcal{H}_{t}(\varepsilon,N)}
	%+\ln{\mathcal{F}}
%\end{equation}
%\begin{equation}
	%\ln{\mathcal{F}_{k+1}}=\frac{u}{2}\ln{\mathcal{F}_{k}}
%\end{equation}
%for $u$ typically greater than 1 and $\mathcal{F}$ is the modification factor for
%the density of states.  We then accept a new configuration for the density of 
%states with a probability given by the following
%\begin{equation}
	%\mathcal{P}(\varepsilon_i \rightarrow \varepsilon_j) 
	%= \min[1,e^{\ln{\mathcal{H}(\varepsilon_i)}-\ln{\mathcal{H}(\varepsilon_j)}}]
%\end{equation}
%Zhou and Bhatt explored the convergence of the Wang-Landau algorithm and 
%determined that choosing a large modification factor $\mathcal{F}_0 = e^{4}$ and
%rapidly reducing the factor by 10 during each step resulted in reduced statistical 
%error~\cite{zhou2005understanding}.

%\subsubsection{The TMMC Algorithm}

%The Transition Matrix Monte-Carlo algorithm is often used in the family of broad histogram
%methods~\cite{swendsen1999transition}.  In fact, without the idea of a transition matrix
%detailed balance cannot be preserved for the Broad histogram method~\cite{wang1999broad}.  The sampling dynamics
%eventually produce a histogram that is flat when using the transition matrix.  The idea of the 
%transition matrix was first developed in the 1970s by Kieth Hastings~\cite{hastings1970monte}.  We begin
%by first initializing a transition matrix $\mathcal{T}_{ij}=0$ for all energies.  We
%next define the transition matrix and attempt to make a defined move which is either 
%accepted or rejected with an acceptance probability given by
%\begin{equation}
%\mathcal{T}_{ij} = \frac{\mathcal{T}_{d}(\varepsilon_j,-\Delta{\varepsilon})}
%{\sum\limits_{\Delta{\varepsilon}}\mathcal{T}_{d}
%(\varepsilon_j,\Delta{\varepsilon})}
%\end{equation}
%\begin{equation}
%\mathcal{P}(\varepsilon_i \rightarrow \varepsilon_j) = 
%\max\bigg[\frac{\mathcal{T}_{d}(\varepsilon_i,\Delta{\varepsilon})
%+\mathcal{C}}{\mathcal{T}_{d}(\varepsilon_f,-\Delta{\varepsilon})
%+\mathcal{C}}\frac{\sum\limits_{\Delta{\varepsilon'}}\mathcal{T}_{d}
%(\varepsilon_i,\Delta{\varepsilon})+\mathcal{C}} 
%{\sum\limits_{\Delta{\varepsilon'}}\mathcal{T}_{d}(\varepsilon_f,
%\Delta{\varepsilon})+\mathcal{C}},\exp{\bigg(-\frac{\Delta{\varepsilon}}
%{k_{B}T}\bigg)}\bigg]
%\end{equation}

%The Multi-Canonical (Flat histogram methods) ensemble is designed in such a way 
%so as to ensure visiting every energy level with equal 
%probability~\cite{swendsen1999transition}.  This would hardly be ideal using a
%Broad histogram approach where the acceptance rate should be related to the sampled
%energy.  While similar to the Multi-Canonical approach in that both methods sample
%over all energy space for a given temperature, the Broad histogram methods have the
%advantage of being able to sample over a larger temperature scale and away from the
%critical point~\cite{Broadhistogram,BroadHistogram2}.  This feature can allow the
%methods to solve more accurately and computationally efficient then Flat histogram
%methods.

%\subsubsection{The OETMMC Algorithm}

%The Optimized Ensemble method has similar features to the TMMC method.  In 
%addition, it re-weights the histogram such that unlikely (not-probable) transitions
%are not equally favored.  This allows the simulation low-energy sampling rate to maximized.
%We outline an approach to updating the weights $\mathcal{W}_{i}$ based on the density
%of states $\mathcal{D}(\varepsilon)$~\cite{trebst2004optimizing,wessel2007optimized}.
%We begin the algorithm by starting with the weights proportional to the reciprocal of
%the density of states.  Next, enter a loop which has the purpose of calculating a fraction
%$\chi(\eta)$ in terms of histograms $\mathcal{H}_{+}(\eta)$ and $\mathcal{H}_{-}(\eta)$ and updating the
%global histogram $\mathcal{H}(\varepsilon)$.
%\begin{equation}
	%\chi = \frac{\mathcal{H}_{+}(\eta)}{\mathcal{H}_{+}(\eta)+\mathcal{H}_{-}(\eta)}
%\end{equation}
%We next update the statistical weights and calculate a new acceptance probability.
%\begin{equation}
	%\mathcal{W}_{i+1}(\eta) = \mathcal{W}_{i}(\eta)\sqrt{\frac{1}{\mathcal{H}_{+}(\eta)+
	%\mathcal{H}_{-}(\eta)}\bigg|\frac{\text{d}\chi(\eta)}{\text{d}\eta}\bigg|} 
%\end{equation}
%Once the statistical weights are found the can relate the density of states and the 
%global histogram. With the completetion of finding the global histogram, thermodynamic
%properties can now be calculated from the density of states.

\bibliography{paper}% Produces the bibliography via BibTeX.

\end{document}
