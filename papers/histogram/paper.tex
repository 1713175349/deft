\documentclass[letterpaper,twocolumn,amsmath,amssymb,pre,aps,10pt]{revtex4-1}
\usepackage{graphicx} % Include figure files
\usepackage{color}
\usepackage{nicefrac} % Include for inline fractions

\newcommand{\red}[1]{{\bf \color{red} #1}}
\newcommand{\green}[1]{{\bf \color{green} #1}}
\newcommand{\blue}[1]{{\bf \color{blue} #1}}
\newcommand{\cyan}[1]{{\bf \color{cyan} #1}}

\newcommand{\davidsays}[1]{{\color{red} [\green{David:} \emph{#1}]}}
\newcommand{\jpsays}[1]{{\color{red} [\blue{Jordan:} \emph{#1}]}}

\begin{document}
\title{Comparing broad histogram methods using the square-well liquid}

\author{Jordan K. Pommerenck} \author{Michael A. Perlin}
\author{Tanner T. Simpson} \author{David Roundy}
\affiliation{Department of Physics, Oregon State University,
  Corvallis, OR 97331}

\begin{abstract}
  We present several histogram methods and compare the performance and efficiency at treating the square-well fluid.
\end{abstract}

\maketitle

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-E}
  %\caption{A histogram plot demonstrating the difficulty of using
    %canonical Monte Carlo to simulate a system.  A simulation at a
    %give temperature only provides information fo a small number of
    %energy states.\label{fig:histograms}}
%\end{figure}

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-weights}
  %\caption{The weight functions from different methods.}
%\end{figure}

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-dos}
  %\caption{A density of states plot demonstrating the difficulty of
    %using canonical Monte Carlo to simulate a system.  A simulation at
    %a give temperature only provides information fo a small number of
    %energy states.\label{fig:dos}}
%\end{figure}

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-sample-rate}
  %\caption{Inverse sampling rates from different methods.}
%\end{figure}

% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-scaling}
%   \caption{Scaling.\label{fig:scaling}}
% \end{figure}

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-u}
  %\caption{Specific internal energy.\label{fig:u}}
%\end{figure}

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-hc}
  %\caption{Specific heat capacity.\label{fig:hc}}
%\end{figure}

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-S}
  %\caption{Specific entropy.\label{fig:S}}
%\end{figure}

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-u_err}
  %\caption{Specific internal energy.\label{fig:u}}
%\end{figure}

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-hc_err}
  %\caption{Specific heat capacity.\label{fig:hc}}
%\end{figure}

%\begin{figure}
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-s_err}
  %\caption{Error in specific entropy.\label{fig:Serr}}
%\end{figure}

%% \begin{figure}
%%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-rdf}
%%   \caption{Radial distribution function.\label{fig:rdf}}
%% \end{figure}

%As shown in Fig.~\ref{fig:histograms}, there are several difficulties
%encountered when running Monte Carlo simulations at a fixed
%temperature.  On the top plot, which shows three simulations run at
%$\lambda = 1.3$, it is apparent that each given canonical simulation
%only provides statistical information for a small number of energy
%states.  The bottom plot shows an even more serious issue: when
%running a simulation at a fixed temperature, it is possible to become
%frozen in a state that is far from the ground state, as happened in
%the $k_{B}T=0.1\epsilon$ simulation.

% \begin{table*}
% \input{figs/scaling-table-ww130-ff30}
% \caption{Scaling data}
% \end{table*}

\section{Introduction}

We consider the square-well fluid i.e. a system of particles whose interactions are governed by a square-well potential~\cite{singh2003surface, barker2004perturbationSW}.  The square-well potential is an ideal testbed as it is the simplest model that ensures both attractive and repulsive forces are experienced by interacting particles~\cite{barker1967-SW-perturbation, vega1992phase}.  The potential $U(\textbf{r})$ for such a system is given by
\begin{equation}
 U(\textbf{r})=\begin{cases} \infty & 
 \lvert\textbf{r}\rvert< \sigma\\-\epsilon & 
 \sigma<\lvert\textbf{r}\rvert<\lambda\sigma\\0 & 
 \lvert\textbf{r}\rvert > \lambda\sigma\end{cases}
\end{equation}
where $\sigma$ is the hard-sphere diameter of the particle, $\lambda$ is the
reduced range of the potential well, and $\epsilon$ is its depth.

The organization of this paper is as follows: In the next section we describe in detail seven flat-histogram methods used to calculate the density of states for the liquid-vapor system.  Section III contains the results and comparison of the methods applied to the square-well fluid.  Section IV is the conclusion as well as an overview of which methods performed well and suggestions for continued improvement. 


\section{Methods}

Here we employ a variety of flat-histogram methods.  We outline the general workings of algorithm we developed in detail and summarize algorithms that can be found elsewhere.  The following alogrithms are introduced and applied to the square-well fluid: Transition Matrix Initialization (TMI), Transition Optimized Ensemble (TOE), Transition Matrix Monte-Carlo (TMMC), Wang-Landau (WL), Wang-Landau Transition Matrix Monte-Carlo (WL-TMMC), Stochastic Approximation Monte-Carlo (SAMC), and Stochastic-Approximation Transition Matrix Monte-Carlo (SA-TMMC).

\subsection{TMI2 Algorithm}

The second version of transition matrix initialization algorithm works
by looking at the logarithm of the density of states.  We begin at
high energies ($\epsilon_{S_{\max}}$ the energy that maximizes the entropy) and work our way
downward.

\paragraph{Reaching $T_{\min}$:}
If the change in the logarithm of the density of states is greater
than the reciprocal of the minimum temperature, i.e. if
\begin{align}
  \ln\frac{D(\epsilon_i)}{D(\epsilon_{i-1})} &>
  \frac{\epsilon_{i-1} - \epsilon_i}{k_BT_{\min}}
  \\
  \frac{D(\epsilon_i)}{D(\epsilon_{i-1})} &>
  e^{\beta_{\max}(\epsilon_{i-1} - \epsilon_i)}
\end{align}
then the algorithm has reached the minimum temperature of interest.
In this case, we set the log weights at all lower energies using a
Boltzmann ratio relative to this energy.
\paragraph{Confident at $\epsilon_i$:} We are confident in our density
of states at energy $\epsilon_i$ provided two conditions are true.
\begin{enumerate}
\item The density of states must be decreasing as the energy decreases
  from its next-higher energy.
\item The change in density of states (relative to the next higher
  energy) must be smaller than a pessimistic estimate of the counting
  uncertainty at the energy we are examining:
  \begin{align}
    \frac{D(\epsilon_i)}{D(\epsilon_{i-1})} < \frac1{\sqrt{N_i}}
  \end{align}
  where $N_i$ is the number of ``pessimistic
  samples'' at $\epsilon_i$, the lower of the two energies under
  consideration.
\end{enumerate}
If both of these are true, we believe we know the density of states at
this energy well, and the weights are set by
\begin{align}
  \ln w(\epsilon_i) = -\ln{D}(\epsilon_i)
\end{align}
\paragraph{Reached $T_\text{converged}$:}
If this is not the case, $\epsilon_i$ is the lowest energy that is
adequately converged.  We set the weights for energies lower than this
to a Boltzmann ratio with a temperature determined using the slope of
a secant line on the $\ln w$ versus $\epsilon$ graph.
\begin{align}
  \beta_\text{secant} = \bigg(\frac{\ln{D}(\epsilon_{S_{\max}}) - \ln{D}
  (\epsilon_\text{pivot})}{\epsilon_{S_{\max}}-\epsilon_\text{pivot}}\bigg)
\end{align}
This ``slope'' is a positive quantity.  The weights at energies below
$\epsilon_\text{pivot}$ are given by
\begin{align}
  \ln w(\epsilon) = \ln w(\epsilon_\text{pivot}) -
  \beta_\text{secant}(\epsilon - \epsilon_\text{pivot})
\end{align}
indicating that there are higher weights at lower energies, since
$\beta$ is positive.  We have one additional caveat: we examine the
lower energies for which we have samples (``pessimistic'' samples),
and if we find any energy at which the density of states is greater
than the inverse of the weight, we adjust $\beta$ to cause it to pass
through that point.  This ensures that the product of the weight with
the estimated density of states is always less than or equal one for
energies below $\epsilon_\text{converged}$.



\subsection{TMI3 Algorithm}
The TMI$_3$ algorithm is like the TMI$_2$ algorithm, with one
difference, which is that instead of a secant line being used to
estimate the converged temperature, we use a tangent line, estimated
using the lowest two ``converged'' energies.




\subsection{TOE3 Algorithm}
The TOE$_3$ method desribed by Trebst et. al.~\cite{trebst2004optimizing} uses the metric of round trips defined by a walker visiting the extreme energies for the density of states.  The rate for the walker is dependent of the diffusivity $D(\epsilon)$ at a given energy $E$.  The goal of the method is to maximize the number of round trips that the Monte-Carlo simulation makes.  The optimized density of states is given by the following:
\begin{align}
  n_{w}^{(opt)} = \frac{1}{\sqrt{D(E)}}
\end{align}
where unlike in the original paper detailing the method, we compute the difusivity directly from the transition matrix normalized with canonical weights.  The optimal ensemble therefore relates the density of states to the inverse of the square root of the difusivity.
\begin{align}
  D(E) = \mid (T(\Delta\epsilon)^2 - (T(\Delta\epsilon))^2) \mid
\end{align}





\subsection{TMMC Algorithm}
The Transition Matrix Monte-Carlo (TMMC) method as introduced by Fitzgerald et al.
consists of three primary steps.  The first two steps are common to the Metropolis 
algorithm while the third makes use of the actual transition 
probabilities~\cite{fitzgerald2000monte}.

\subsubsection{define a transition probability}
Assuming that the system under study is in a state denoted old and a proposed transition
is made to a new state denoted new, the probability is defined as
\begin{align}
  p_{old \rightarrow new} = p_{new \rightarrow old}
\end{align}
where we have allowed the probabilies to be equal for simplicity of presentation.

\subsubsection{define an acceptance probability}
The probability for accepting a transition from an old state to a new one can be 
defined
\begin{align}
  p_{a} = \min\bigg[1,\frac{\alpha_{new\rightarrow old}}
  {\alpha_{old \rightarrow new}}\frac{\pi_{new}}{\pi_{old}}\bigg]
\end{align}
where $\alpha_{new\rightarrow old}$ is the probability of generating a 
new system configuration the old.  For conventional Monte-Carlo moves 
$\alpha_{old \rightarrow new} =\alpha_{new\rightarrow old}$ although 
for advanced Monte-Carlo moves this would not be the 
case~\cite{paluch2008comparing, siepmann1990method}.  The probabily of 
observing the system in the old state can be found based on the 
ensemble constraints placed on the system.  Fitzgerald et al. 
considered the system of interest using the canonical ensemble however extension to other ensembles is quite natural.  
\begin{align}
  \pi_{old} = \frac{e^{-\beta E_{old}}}{Z}
\end{align}
The acceptance probability given for TMMC is idential to that introduced by 
Swendsen and Wang~\cite{wang2002transition}.
\begin{align}
  p_{a} = \min\bigg[1,e^{-\frac{\delta\epsilon}{k_{B} T}}\bigg]
\end{align}
\subsubsection{define a bookkeeping step}
The third step in the TMMC algorithm consists of recording the data in a collection 
matrix (analogous to the transition histogram matrix).
\begin{align}
  C_{N,\delta} = C_{N,\delta} + p_{a}\\
  C_{N,0} = C_{N,0} +(1 - p_{a})
\end{align}  
If $\delta=0$, that is, the energy state has not been changed, then only 
$C_{N,0}$ is incremented by unity. Fitzgerald et al.~\cite{fitzgerald2000monte}
show that defining the third step this way results in a uniform improvement over 
using histogram estimators to determine the PNPD.




\subsection{Wang-Landau Algorithm}

The Wang-Landau algorithm is a powerful re-weighting Histogram method 
developed and refined well into 21st 
century~\cite{wang2001determining}.  While methods such as Metropolis 
Sampling and Swendsen-Wang cluster 
flipping~\cite{swendsen1987nonuniversal} generate a narrow distribution 
and require sampling at individual temperatures, the Wang-Landau 
algorithm uses a flat histogram and performs a random walk in energy 
space to determine the density of states~\cite{LandauMinSampling}.  
Wang-Landau's major tenant is that when counting the histogram 
$\mathcal{H}$($\varepsilon$), the energy occurences should form a flat 
distribution.  The criteria for flat sampling is given by 
\begin{equation}
	\frac{\min_{\varepsilon} \mathcal{H}(\varepsilon)}
	{\big\langle\mathcal{H}(\varepsilon)\big\rangle } 
	> \gamma 
\end{equation}
where $\gamma$ is usually set between 0.75 and 0.99 and is determines how many
times each energy is sampled relative to the mean number of visits.  Difficulties
can occur with this flatness criteria due to the fact that some energies on a
given energy range might never be sampled~\cite{haber2014transition}.  The algorithm
is briefly outlined below  
\begin{equation}
	\ln{\mathcal{H}_{t+1}(\varepsilon,N)}=\ln{\mathcal{H}_{t}(\varepsilon,N)}
	+\ln{\mathcal{F}}
\end{equation}
\begin{equation}
	\ln{\mathcal{F}_{k+1}}=\frac{u}{2}\ln{\mathcal{F}_{k}}
\end{equation}
for $u$ typically greater than 1 and $\mathcal{F}$ is the modification factor for
the density of states.  We then accept a new configuration for the density of 
states with a probability given by the following
\begin{equation}
	\mathcal{P}(\varepsilon_i \rightarrow \varepsilon_j) 
	= \min[1,e^{\ln{\mathcal{H}(\varepsilon_i)}-\ln{\mathcal{H}(\varepsilon_j)}}]
\end{equation}
Zhou and Bhatt explored the convergence of the Wang-Landau algorithm and 
determined that choosing a large modification factor $\mathcal{F}_0 = e^{4}$ and
rapidly reducing the factor by 10 during each step resulted in reduced statistical 
error~\cite{zhou2005understanding}.


\subsection{WL-TMMC Algorithm}

Wang-Landau Transition Matrix Monte-Carlo (WL-TMMC) method as proposed 
by Shell and coworkers~\cite{shell2003improved,shell2004flat} takes 
advantage of the ease of implementation and applicability to a variety 
of systems, while making use of a transition matrix to adress WL 
weaknesses such as a limited statistical accuracy plateau which is not 
improved with additional Monte-Carlo steps.  This WL version is more 
accurate for a given number of simulation steps.

\paragraph{Initializing Wang-Landau:} We begin the simulation by 
defining a minimum important energy and a maximum entropy state. In 
traditional Wang-Landau the WL factor begins at 1.0 and the factor is 
modified by 2.0 until the simulation is cutoff typically around 
$10^{-10}$.  Rane et. al. in the first published implemenation of 
WL-TMMC~\cite{rane2013monte} suggests using a WL factor between 1.0 and 
$10^{-2}$ with a cutoff $<10^{-5}$. In our implementation of WL-TMMC, 
we set the WL factor to be 1 and the cutoff at $10^{-4}$. A single 
sweep is defined as each macrostate being sampled a number of times in 
this case 10,000 (Siderius et. al. does 100). Wang-Landau sets a 
flatness criteria~\cite{wang2001determining, wang2001efficient, 
hatch2015computational, mahynski2017predicting} for the accumulated 
histogram usually between 0.75 to 0.90.  For our simulation, it is 
sufficient that each macrostate is visited at least once (each 
histogram bin has one entry)~\cite{shell2003improved}.  The goal of 
this WL initialization is to prefill the transition matrix for the next 
portion of the simulation. 
\paragraph{Initializing TMMC:} The purpose of prefilling the transition 
matrix is apparent because if numerous zeros existed in the collection 
matrix, the infinite temperature transition matrix would be 
ill-defined.  Sampling over large ranges of density of states would 
therefore take an unacceptable length of time to 
complete~\cite{shell2003improved, shen2014elucidating}.  The transition 
matrix in terms of the collection matrix is given as follows:
\begin{align}
\widetilde{T}_{\infty}(I\rightarrow J) = \frac{C(I,J)}
{\sum_{K} C(I,K)}
\end{align}
We now run the TMMC portion of the simulation.  Siderius does this for
20 additional sweeps (does about 8 sweeps for WL).  We do this until we
reach a specified temperature of interest.
We have not yet implemented a biasing function to control acceptance
rate of trial moves but we probably could/should? at some point.
\begin{align}
  P_{acc}(i\rightarrow j) = \min\bigg[1,\frac{\widetilde{T}_{\infty}(J\rightarrow I)}
  {\widetilde{T}_{\infty}(I\rightarrow J)}\bigg]
\end{align}
this is only used to control actual transitions not update the collection
matrices.




\subsection{SAMC Algorithm}
The SAMC Algorithm was proposed as the reason that the Wang-Landau Algorithm converges.  It is primarily the 1/t WL Algorithm although it does not require knowledge of the entire energy range.


\subsection{SA-TMMC Algorithm}
We introduce a new algorithm consisting of the merger of the Stochastic Approximation (SA) method proposed by Liang et al.~\cite{liang2007stochastic} and the TMMC Algorithm with suitable adaptations.  While the Stochastic Approximation method has recently been implemented in comparison with the Wang-Landau Algorithm~\cite{werlich2015stochastic, schneider2017convergence}, it seems quite natural to create a new Monte-Carlo method which we will call SA-TMMC.  Like in the original implementation, we define an update factor where $t_0$ is the number of Monte-Carlo moves and $t$ is the current iteration.  
\begin{align}
F_{t}^{\text{SA}} = \frac{t_0}{\max(t_0,t)}
\end{align}
One potential problem of merging SA and TMMC is that SA provides no nice transition to the later method due to the lack of a user-controlled cutoff.  Also for SA Monte-Carlo, the choice of parameter $t_0$ proves difficult particularly for complex systems.  Finally, we enforce a minimum temperature probability which prevents the system from sampling states below the minimum temperature.  We discuss here how we resolve all of the potential issues we encountered when integrating both SA and TMMC. 

\subsubsection{Define an adaptive parameter $t_0$}
It has been shown that if $F_{t}^{\text{SA}}$ satisfies two conditions the estimator is proven to converge to the real DOS~\cite{liang2006theory, liang2007stochastic}.
\begin{align}
\sum_{t=1}^\infty F_{t}^{\text{SA}} = \infty \quad\textrm{and}\quad
\sum_{t=1}^\infty (F_{t}^{\text{SA}})^\zeta < \infty
\end{align}
where $\zeta \in \{1,2\}$.  For practical implementation with TMMC, we find it necessary to reset $t_0$ everytime a new energy state is found.  This ensures that all energy states are sampled before TMMC move probabilities begin to be favored.  
\jpsays{I have no idea how this impacts convergence!}

\subsubsection{Define a statistical weight to control the transition between SA and TMMC}
Our algorithm can alternate between SA and TMMC on the fly using weights to determine the probability that a move between adjacent energy states is allowed.  We define the transition probability $\text{SA}_{\text{Weight}}$ as
\begin{align}
\text{SA}_{\text{Weight}} =\frac{\sqrt{t_0}}{N_\text{found}} 
\sqrt{\frac{1}{n_\downarrow}+\frac{1}{n_\uparrow}}
\end{align}
where $t_0$ is the number of Monte-Carlo moves, $N_\text{found}$ is the number of energy states that have been found i.e. visited, and $n_{\uparrow\downarrow}$ are the number of upward or downward moves between adjacent energy states respectively.  The rational behind the transition probability is that the transition weight can be related to the fractional uncertainty in upward and downward moves assuming there is no system correlation (the moves are independent).  Although this is not necessarily the case, we find that this definition for the transition weight is satifactory.  The prefactor $\nicefrac{\sqrt{t_0}}{N_\text{found}}$ is justified by the fact that $t_0$ represents how many moves were necessary to randomly sample $N_\text{found}$ energies and $N^2_\text{found}$ is the number of samples necessary to randomly sample all the energies independently.  

\subsubsection{Enforce a minimum temperature probability}
One potentially damaging aspect to the convergence of the Stochastic Approximation method is the need to enforce a minimum temperature probability.  We define this probability as 
\begin{align}
P_{\text{min}} = e^{\beta_{\text{max}}\Delta\epsilon}
\end{align}
where $\beta_{\text{max}} = \nicefrac{1}{k_B T_\text{min}}$.  If the transition probability is ever below the minimum temperature transition probability, we set the former equal to the later.

\section{Results}

Talk about results of simulations and show graphics here...


\section{New ideas for algorithms}

\subsection{Finding maximum probability state}

Technically maximum histogram or maximum entropy.

Loop:

run iterations until we see $N$ energy changes, counting the number of
returns to the initial energy.  If there are sufficient returns, then
we are at the maximum, where sufficient returns will be determined by
a random walk (weighted?) where we compute the expected number of
returns if the walk is not weighted.  We may want to correct to get
all the way to the top.

If we are not at the top yet, we try again.  Eventually we will be.

\subsection{Finding variance of the histogram}

I assume we first ran the max-entropy/histogram algorithm, which gives
us a maximum value of $E_0$.  We then zero the histogram.

We run for a short while, and then compute the mean and standard
deviation of the energy from the histogram: $\bar E$ and $\sigma$.
While doing this, we track the number of returns to the initial state
$n$.  Now the uncertainty in the mean is given by
\begin{equation}
  \Delta \bar E \approx \frac{\sigma}{\sqrt{n}}
\end{equation}
if $\Delta \bar E \ll (E_0 - \bar E)$ then we can conclude that we are
certain where the maximum is, and how wide it is.

%\begin{figure*}
  %\includegraphics[width=0.33\textwidth]{figs/periodic-ww130-ff30-N20-tmmc-golden-transitions}\hfill%
%\includegraphics[width=0.33\textwidth]{figs/periodic-ww130-ff30-N20-tmmc-transitions}\hfill%
%\includegraphics[width=0.33\textwidth]{figs/periodic-ww130-ff30-N20-tmmc-golden-tmmc-compare-transitions}
  %\caption{A plot showing the probability of energy transitions (with
    %no weighting) from given energy states.\label{fig:transitions}}
%\end{figure*}

\subsection{Tracking transitions}

We can track the number of attempted transitions from each energy
eestate to each other energy state.  This statistic has the advantage of
being approximately independent of the weighting function used
(although it is highly dependent on the step size).  Thus we could
continue to refine this statistic, even as we are updating the weight
array.  Here are two papers that seem highly relevant to this
method~\cite{wang1999transition, wang2002transition}, but I'm not sure
if they're identical.

The ratio of density of states at two energies should be proportional
to the ratio of upgoing and downgoing transition rates between those
two energies.  Thus these transitions give us information about
$\Delta$$\mathcal{D}$($\varepsilon$) (where $\mathcal{D}$($\varepsilon$) 
is the density of states).

Figure~\ref{fig:transitions} below shows the transition matrix for a
20-sphere system, computed during initialization using the Wang-Landau
algorithm.

How can we use this?

\subsubsection{Generating $w$ from the transition matrix}

We should be able to generate a set of weighting functions directly
from the transition matrix.  We could do this either by optimizing for
a flat histogram, or by optimizing the round-trip rate.  Both would be
interesting to implement.

One natural approach would use linear algebra.  If the normalized
transition matrix is $\mathcal{T}$, then the density of states is given by
\begin{equation}
  \mathcal{D}(\varepsilon) = \mathcal{T}\mathcal{D}(\varepsilon) 
\end{equation}
By solving this equation for $\mathcal{D}$($\varepsilon$), we could find a 
good approximation for $w$ to obtain a flat histogram.

I expect that we could use similar math to the optimized ensemble
folks to optimize the round-trip rate.

\subsubsection{Initializing using the transition matrix directly}

An alternative initialization algorithm involves using the transition
matrix directly during initialization.  This process was proposed by
Swendsen \emph{et al.} as a modification for their initialization
procedure~\cite{swendsen1999transition}.  In this case, the transition
matrix is continually updated, thus modifying the relative weighting
of different energies as the simulation proceeds.  This strictly
speaking no longer satisfies detailed balance, but as the simulation
proceeds this violation will quickly become increasingly small.

This raises the question of how to use the transition matrix to
directly determine transition probabilities, since the self-consistent
eigenvalue solution is unsuitable.  The key is to recognize that we do
not require weights for each energy but only \emph{differences} of
weights for on pair of energies at a time.  This is a much smaller
problem.

Swendsen \emph{et al.} find for a flat distribution, the acceptance
rate for a given transition is equal to the ratio of that element in
the transition matrix to the element corresponding to the reverse
transition~\cite{swendsen1999transition}.  Naturally, for a different
distribution, we can scale the acceptance ratios according to the
ratio of occupations desired.  We modify this algorithm to always
accept transitions to a lower energy, thus employing a broad histogram
method that does not much sample energies above the maximum-entropy
energy.

Swendsen only uses this approach after a slightly hokey two-stage
initialization.  It seems to work pretty well, and asymptotically it
does have detailed balance.

\newpage
\jpsays{Everything below this is TMI1 and GC-TMMC}
\subsection{}
%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-u_errors}
  %\caption{$u$ error scaling.}
  %\label{fig:scaling-u_err}
%\end{figure}
%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-cv_errors}
  %\caption{$c_V$ error scaling.}
  %\label{fig:scaling-cv_err}
%\end{figure}
%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-s_errors}
  %\caption{$s$ error scaling.}
  %\label{fig:scaling-s_err}
%\end{figure}

%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-u_error_comp}
  %\caption{$u$ error comparisons.}
  %\label{fig:scaling-u_err_comp}
%\end{figure}
%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-cv_error_comp}
  %\caption{$c_V$ error comparisons.}
  %\label{fig:scaling-cv_err_comp}
%\end{figure}
%\begin{figure}[p]
  %\includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-s_error_comp}
  %\caption{$s$ error comparisons.}
  %\label{fig:scaling-s_err_comp}
%\end{figure}


% \subsection{Convergence tests}

% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-dos-conv-T10}
%   \caption{Convergence}
%   \label{fig:conv-dos}
% \end{figure}

% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-u-conv-T10}
%   \caption{Convergence}
%   \label{fig:conv-u}
% \end{figure}

% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-hc-conv-T10}
%   \caption{Convergence}
%   \label{fig:conv-hc}
% \end{figure}
% \begin{figure}
%   \includegraphics[width=\columnwidth]{figs/periodic-ww130-ff30-N20-S-conv-T10}
%   \caption{Convergence}
%   \label{fig:conv-S}
% \end{figure}

%\subsection{TMI1 Algorithm}

%Our proposed transition matrix initilization algorithm TMI$_1$ uses the following 
%method to determine the histogram weights.  The method periodically recomputes 
%the weights during the simulation.  Although no bottlenecks result when updating 
%the weights more frequently, the algorithm will nonetheless introduce additional 
%computational time.  Because of the potential to increase computational time, the 
%algorithm infrequently but periodically updates the weights such that the central
%effort of the method allows focusing efforts on exploring new energy
%configurations.

%\subsubsection{Weights at High Energy}
%At energies above the maximum in the density of states, the method sets the weights 
%equal to the weight at the density of states maximum.  This avoids expending 
%extra computational time on equally sampling states that are not favored at any 
%positive temperature.

%\subsubsection{Weights at Interesting Energy}
%At energies below the maximum, the method attempts to set the weight equal to the 
%inverse of the density of states, \emph{unless} there are insufficient statistics 
%to justify this.  In order to determine when sufficient statistics have been reached,
%it becomes neccessary to define a modifcation factor, in this case, in terms of a 
%fractional uncertainty
%\begin{align}
  %{F}(\epsilon) &= \frac{1}{\sqrt{N_s(\epsilon)}}
%\end{align}
%where $N_s(\epsilon)$ is the number of round-trip samples taken at an energy
%$\epsilon$.  We assume that our uncertainty in the ratio between the density
%of states at two adjacent energies is equal to this value.  The weight at each
%energy can then be set according to
%\begin{align}
  %{W}_{\epsilon} &= {W}_{\epsilon+1}\max\left[\frac1{{F}
  %(\epsilon)},\frac{{D}(\epsilon)}{{D}(\epsilon+1)}\right]
  %\\
   %&= {W}_{\epsilon+1}\max\left[\frac1{\sqrt{N_s(\epsilon)}},
  %\frac{{D}(\epsilon)}{{D}(\epsilon+1)}\right]
%\end{align}

%\subsubsection{Weights at Low Energy}
%We define a minimum interesting temperature $T_{\min}$, and at low
%energies do some special stuff to avoid weighting things more
%strongly than a Boltzmann factor at $T_{\min}$ would do.

%\subsection{GC-TMMC Algorithm}
%Grand Canonical Transition Matrix Monte-Carlo (GC-TMMC) allows the energy and 
%particle number of the system to fluctuate while holding the chemical potential,
%volume, and temperature constant~\cite{chen2001aggregation, chen2002simulating, 
%errington2003direct, fenwick2006accurate, fenwick2008direct, errington2005direct}.  
%The algorithm boasts a lower statistical noise than conventional GCMC due to its 
%incorporation of a transition matrix to estimate ensemble averages~\cite{siderius2013use, 
%paluch2008comparing, grzelak2008computation}.  The steps for the algorithm are 
%outlined below:

%\subsubsection{Define particle insertion/deletion}
%In the system of consideration, the number of particles $\mathcal{N}$ can change
%by an amount $\delta$.  It is assumed that in the system either no particles can be 
%inserted or deleted, a single insertion takes place, or a single deletion takes place.
%This yields a condition $\delta={-1,0,+1}$ for particle insertion.

%\subsubsection{Define an acceptance probability}
%Now it is necessary to define criteria for determining whether a particle insertion/deletion
%is allowed.  This criteria is called the acceptance probability and is denoted as
%\begin{align}
  %p_{a} = \min\bigg[1,\frac{\alpha_{new\rightarrow old}}
  %{\alpha_{old \rightarrow new}}\frac{\pi_{new}}{\pi_{old}}\bigg]
%\end{align}
%where $\alpha_{new\rightarrow old}$ is the probability of generating an old system 
%configuration from the new.  For conventional Monte-Carlo moves $\alpha_{old \rightarrow new}
%=\alpha_{new\rightarrow old}$ although for advanced Monte-Carlo moves this would not 
%be the case~\cite{siepmann1990method}.  The probabilies of actually observing the 
%system in the old state is given by
%\begin{align}
  %\pi_{old} = \frac{1}{\Xi}\frac{V^{N_{old}}}{\Lambda^{3N_{old}}N_{old}!}
  %e^{-\beta(\epsilon_{old}-\mu N_{old})}
%\end{align}  
%where $\Xi$ is the grand canonical partition function and $\Lambda$ is the De Broglie
%wavelength.

%\subsubsection{Define a Collection Matrix}
%In order to calculate an overall transition probability $\mathcal{P}_{N,\delta}$, we
%must first define how the system procession is to be recorded.  A Collection Matrix
%can be defined, such that for any one move, two elements are updated.
%\begin{align}
  %{C}_{N,\delta} = {C}_{N,\delta} + p_{a}\\
  %{C}_{N,0} = {C}_{N,0} +(1 - p_{a})
%\end{align}  
%If $\delta=0$, that is, the number of particles has not been changed, then only 
%$\mathcal{C}_{N,0}$ is incremented by unity.  The overal transition probability can
%now be defined as 
%\begin{align}
  %\mathcal{P}_{N,\delta} = \frac{{C}_{N,\delta}}
  %{\sum_{j=-1}^{+1} {C}_{N,j}}
%\end{align} 

%\subsubsection{Define the Particle Number Probability Distribution}
%The Particle Number Probability Distribution (PNPD) $\Pi(N;\mu,V,T)$ can be found 
%by enforcing detailed balance in the system.
%\begin{align}
  %\ln(\Pi_{N+1}) = \ln(\Pi_{N}) + \ln\bigg(\frac{\mathcal{P}_{N\rightarrow N+1}}
  %{\mathcal{P}_{N+1\rightarrow N}}\bigg)
%\end{align} 
%This equation will hold as long as N-space can be sampled sufficiently.  In an 
%implemented algorithm $N$, $N_{min}$, and $N_{max}$ are specified for macrostate domain
%sampling.

%\subsubsection{Define a biasing function}
%The primary difference between TMMC as defined by Fitzgerald et al. and GC-TMMC as 
%pioneered by JR Errington et al. is the use of a biasing function to control the acceptance
%rate of trial moves~\cite{siderius2013use, fitzgerald2000monte}.  This type of 
%multi-canonical sampling augments the acceptance probability in the following way 
%\begin{align}
  %p_{\eta,old\rightarrow new} = \min\bigg[1,\frac{e^{\eta(N_{new})}}
  %{e^{\eta(N_{old})}}\frac{\alpha_{new\rightarrow old}}
  %{\alpha_{old \rightarrow new}}\frac{\pi_{new}}{\pi_{old}}\bigg]
%\end{align}

%Although $p_{\eta}$ is used to control the actual transitions for trial configurations
%in GC-TMMC, $p_{a}$ is still used to update the Collection Matrices.  Ideally the biasing
%function would be given by 
%\begin{align}
  %\eta(N) = -\ln\Pi(N;\mu,V,T)
%\end{align}
%Since $\Pi$ is not usually known a priori, the biasing function is usually set to some 
%arbitrary constant for initialization.  The goal of this biasing function is to achieve
%uniform sampling (a common feature shared by the flat histogram Monte-Carlo family).

%\subsubsection{Histogram reweighting for the PNPD}
%Once the PNPD has beeen collected for a given chemical potential $\mu_0$, histogram 
%reweighting is used to determine thermodynamic properties at other chemic potentials.  
%Since the canonical partition function $Z(N,V,T)$ does not depend on the chemical 
%potential, we can write
%\begin{align}
%\begin{split}
  %\Pi(N;\mu,V,T) = \frac{e^{\beta\mu N}Z(N,V,T)}{\Xi(\mu,V,T)}\\
  %\ln\Pi(N;\mu,V,T) = \ln\Pi(N;\mu_0,V,T) \\
  %+ \beta N(\mu-\mu_0) + C
%\end{split}
%\end{align}
%where $C$ is a normalization constant independent of the number of particles.


\bibliography{paper}% Produces the bibliography via BibTeX.

\end{document}
